# Fine-tuning ê°€ì´ë“œ

ë°˜ë„ì²´ ALD ê³µì • ì „ë¬¸ê°€ ëª¨ë¸ì„ Fine-tuningí•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
finetuning/
â”œâ”€â”€ README.md              # ì´ íŒŒì¼
â”œâ”€â”€ data/                  # í•™ìŠµ ë°ì´í„°
â”‚   â”œâ”€â”€ train.jsonl       # í•™ìŠµìš© Q&A ìŒ
â”‚   â””â”€â”€ eval.jsonl        # ê²€ì¦ìš© Q&A ìŒ
â”œâ”€â”€ models/                # Fine-tuned ëª¨ë¸ ì €ì¥ì†Œ
â”‚   â””â”€â”€ qwen-ald-lora/    # LoRA ì–´ëŒ‘í„° ëª¨ë¸
â””â”€â”€ scripts/               # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
    â”œâ”€â”€ prepare_finetuning_data.py  # ë°ì´í„° ìƒì„±
    â”œâ”€â”€ finetune_llama.py           # Fine-tuning ì‹¤í–‰
    â””â”€â”€ run_finetuning.sh           # ì „ì²´ ìë™ ì‹¤í–‰
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ìë™ ì‹¤í–‰ (ê¶Œì¥)

```bash
cd /home/keti_spark1/ald-rag-lab
./finetuning/scripts/run_finetuning.sh
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. í•™ìŠµ ë°ì´í„° ìƒì„±
2. Fine-tuning ì‹¤í–‰
3. ëª¨ë¸ ì €ì¥

### ìˆ˜ë™ ì‹¤í–‰

#### 1ë‹¨ê³„: í•™ìŠµ ë°ì´í„° ìƒì„±

```bash
cd /home/keti_spark1/ald-rag-lab
python finetuning/scripts/prepare_finetuning_data.py
```

**ìƒì„±ë˜ëŠ” ë°ì´í„°:**
- `docs/docs_ald.json`ì—ì„œ ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„± (ê°œì„ ëœ ë²„ì „)
- `feedback/feedback_data.json`ì—ì„œ ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸ í¬í•¨
- ë” ë‹¤ì–‘í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ íŒ¨í„´ ìƒì„±
- `finetuning/data/train.jsonl`, `eval.jsonl` ìƒì„±

**ë°ì´í„° íŠ¹ì§•:**
- ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ íŒ¨í„´ í¬í•¨ ("ë­ì•¼?", "ì–´ë–»ê²Œ?")
- í‰ê·  ë‹µë³€ ê¸¸ì´: ~90ì
- í‚¤ì›Œë“œ ì¡°í•© ì§ˆë¬¸ í¬í•¨
- í”¼ë“œë°± ë°ì´í„° ë°˜ì˜

#### 2ë‹¨ê³„: Fine-tuning ì‹¤í–‰

```bash
python finetuning/scripts/finetune_llama.py \
  --train_file finetuning/data/train.jsonl \
  --eval_file finetuning/data/eval.jsonl \
  --output_dir finetuning/models/qwen-ald-lora \
  --num_epochs 3 \
  --batch_size 4
```

**íŒŒë¼ë¯¸í„°:**
- `--model_name`: ê¸°ë³¸ ëª¨ë¸ (ê¸°ë³¸ê°’: Qwen/Qwen2.5-7B-Instruct)
- `--num_epochs`: í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸ê°’: 3)
- `--batch_size`: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 4)
- `--learning_rate`: í•™ìŠµë¥  (ê¸°ë³¸ê°’: 2e-4)

#### 3ë‹¨ê³„: Fine-tuned ëª¨ë¸ ì‚¬ìš©

`rag_core.py`ì—ì„œ Fine-tuned ëª¨ë¸ ê²½ë¡œ ì„¤ì •:

```python
FINETUNED_MODEL_PATH = BASE_DIR / "finetuning" / "models" / "qwen-ald-lora"
```

## ğŸ“¦ í•„ìš” íŒ¨í‚¤ì§€

```bash
pip install transformers datasets peft accelerate bitsandbytes
```

## âš™ï¸ ê¸°ìˆ  ìŠ¤íƒ

- **ê¸°ë³¸ ëª¨ë¸**: Qwen/Qwen2.5-7B-Instruct
- **Fine-tuning ë°©ë²•**: LoRA (Low-Rank Adaptation)
- **LoRA ì„¤ì •**:
  - rank (r): 16
  - alpha: 32
  - target_modules: q_proj, v_proj, k_proj, o_proj
  - dropout: 0.1

## âš ï¸ ì£¼ì˜ì‚¬í•­

- **GPU ê¶Œì¥**: 16GB VRAM ì´ìƒ
- **í•™ìŠµ ì‹œê°„**: 2-4ì‹œê°„ (ë°ì´í„°ì…‹ í¬ê¸° ë° GPU ì„±ëŠ¥ì— ë”°ë¼)
- **ë©”ëª¨ë¦¬**: LoRA ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (ì „ì²´ ëª¨ë¸ Fine-tuning ëŒ€ë¹„)
- **ì²´í¬í¬ì¸íŠ¸**: í•™ìŠµ ì¤‘ê°„ ì €ì¥ë³¸ì€ `checkpoint-*/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë¨

## ğŸ“Š ë°ì´í„° í†µê³„

í˜„ì¬ í•™ìŠµ ë°ì´í„°:
- í•™ìŠµ ë°ì´í„°: ~850ê°œ
- ê²€ì¦ ë°ì´í„°: ~215ê°œ
- í‰ê·  ë‹µë³€ ê¸¸ì´: ~90ì
- ì§ˆë¬¸ íŒ¨í„´: ë‹¤ì–‘ (ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ í¬í•¨)

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
- `--batch_size`ë¥¼ ì¤„ì´ê¸° (ì˜ˆ: 2 ë˜ëŠ” 1)
- `gradient_accumulation_steps` ì¦ê°€

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼
- GPU í™•ì¸: `nvidia-smi`
- ë°°ì¹˜ í¬ê¸° ì¡°ì •
- LoRA rank ì¡°ì • (r ê°’ ê°ì†Œ)

### ëª¨ë¸ì´ ì œëŒ€ë¡œ í•™ìŠµë˜ì§€ ì•ŠìŒ
- í•™ìŠµë¥  ì¡°ì • (ì˜ˆ: 1e-4)
- ì—í­ ìˆ˜ ì¦ê°€
- ë°ì´í„° í’ˆì§ˆ í™•ì¸

