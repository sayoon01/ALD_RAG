#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
docs_ald.json ë¬¸ì„œ ê´€ë¦¬ í†µí•© ìŠ¤í¬ë¦½íŠ¸

ê¸°ëŠ¥:
1. ìƒˆ ë¬¸ì„œ ì¶”ê°€ (NEW_DOCS ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš°)
2. ì¤‘ë³µ ì œê±° (ê¸°ì¡´ ë¬¸ì„œ ë‚´ + ìƒˆ ë¬¸ì„œ vs ê¸°ì¡´ ë¬¸ì„œ)
3. í‚¤ì›Œë“œ ê°œìˆ˜ë³„, ì¢…ë¥˜ë³„ ì •ë ¬
4. ID ì¬í• ë‹¹

ì‚¬ìš©ë²•:
- ìƒˆ ë¬¸ì„œ ì¶”ê°€: NEW_DOCS ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì„œ ì¶”ê°€ í›„ ì‹¤í–‰
- ì •ë ¬ë§Œ: NEW_DOCSë¥¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸([])ë¡œ ë‘ê³  ì‹¤í–‰
"""

import json
from pathlib import Path
from typing import List, Dict, Any, Tuple

# íŒŒì¼ ê²½ë¡œ
BASE_DIR = Path(__file__).resolve().parent.parent.parent  # ~/ald-rag-lab
JSON_FILE = BASE_DIR / "docs" / "docs_ald.json"

# ì¶”ê°€í•  ìƒˆ ë¬¸ì„œë“¤ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ì •ë ¬ë§Œ ìˆ˜í–‰)
# ìƒˆ ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ë ¤ë©´ ì•„ë˜ ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ì„¸ìš”
# ì˜ˆì‹œ:
# NEW_DOCS = [
#     {
#         "id": 1,  # idëŠ” ë¬´ì‹œë¨ (ìë™ í• ë‹¹)
#         "keywords": ["ALD"],
#         "text": "ìƒˆë¡œìš´ ë¬¸ì„œ ë‚´ìš©"
#     },
# ]
NEW_DOCS = []


def normalize_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•˜ì—¬ ë¹„êµ (ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜ ë“±)"""
    # ê³µë°± ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
    return text.strip().lower().replace(" ", "").replace("\n", "").replace("\t", "")


def is_duplicate_text(text1: str, text2: str) -> bool:
    """ë‘ í…ìŠ¤íŠ¸ê°€ ì¤‘ë³µì¸ì§€ í™•ì¸"""
    normalized1 = normalize_text(text1)
    normalized2 = normalize_text(text2)
    
    # ì™„ì „ ì¼ì¹˜
    if normalized1 == normalized2:
        return True
    
    # ìœ ì‚¬ë„ê°€ ë†’ì€ ê²½ìš° (85% ì´ìƒ ì¼ì¹˜)
    if len(normalized1) > 0 and len(normalized2) > 0:
        common_chars = sum(1 for c in normalized1 if c in normalized2)
        similarity = common_chars / max(len(normalized1), len(normalized2))
        if similarity > 0.85:
            return True
    
    return False


def is_duplicate(new_text: str, existing_docs: List[Dict[str, Any]]) -> Tuple[bool, int]:
    """ìƒˆ í…ìŠ¤íŠ¸ê°€ ê¸°ì¡´ ë¬¸ì„œì™€ ì¤‘ë³µë˜ëŠ”ì§€ í™•ì¸"""
    for doc in existing_docs:
        if is_duplicate_text(new_text, doc.get("text", "")):
            return True, doc["id"]
    return False, None


def remove_duplicates(documents: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:
    """ê¸°ì¡´ ë¬¸ì„œ ë‚´ì—ì„œ ì¤‘ë³µ ì œê±°"""
    unique_docs = []
    seen_texts = set()
    removed_count = 0
    
    for doc in documents:
        normalized_text = normalize_text(doc.get("text", ""))
        
        # ì´ë¯¸ ë³¸ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        is_dup = False
        for seen_text in seen_texts:
            if is_duplicate_text(normalized_text, seen_text):
                is_dup = True
                break
        
        if not is_dup:
            unique_docs.append(doc)
            seen_texts.add(normalized_text)
        else:
            removed_count += 1
    
    return unique_docs, removed_count


def normalize_keyword(keyword, existing_docs):
    """í‚¤ì›Œë“œë¥¼ ê¸°ì¡´ í˜•ì‹ì— ë§ê²Œ ì •ê·œí™”"""
    kw_lower = keyword.lower()
    
    # ê¸°ì¡´ ë¬¸ì„œì—ì„œ ê°™ì€ í‚¤ì›Œë“œ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    for doc in existing_docs:
        for ekw in doc.get("keywords", []):
            if ekw.lower() == kw_lower:
                # ê¸°ì¡´ í‚¤ì›Œë“œ í˜•ì‹ ë°˜í™˜
                return ekw
    
    # ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
    return keyword


def merge_documents():
    """ë¬¸ì„œ ë³‘í•© ë° ì •ë ¬ ì‹¤í–‰"""
    # ê¸°ì¡´ íŒŒì¼ ì½ê¸°
    if not JSON_FILE.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {JSON_FILE}")
        return
    
    with open(JSON_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # ì›ë³¸ ë°ì´í„° ë°±ì—…ìš© ë³µì‚¬
    original_data = json.loads(json.dumps(data))
    
    existing_docs = data.get("documents", [])
    
    print(f"ğŸ“„ ê¸°ì¡´ ë¬¸ì„œ ìˆ˜: {len(existing_docs)}")
    
    # ì¤‘ë³µ ì œê±° (ê¸°ì¡´ ë¬¸ì„œ ë‚´)
    print("ğŸ” ê¸°ì¡´ ë¬¸ì„œ ë‚´ ì¤‘ë³µ ê²€ì‚¬ ì¤‘...")
    unique_docs, removed_count = remove_duplicates(existing_docs)
    
    if removed_count > 0:
        print(f"ğŸ—‘ï¸  ì¤‘ë³µ ë¬¸ì„œ {removed_count}ê°œ ì œê±°ë¨")
        print(f"ğŸ“„ ì¤‘ë³µ ì œê±° í›„ ë¬¸ì„œ ìˆ˜: {len(unique_docs)}ê°œ")
    else:
        print("âœ… ê¸°ì¡´ ë¬¸ì„œ ë‚´ ì¤‘ë³µ ì—†ìŒ")
    
    existing_docs = unique_docs
    
    # ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì—¬ë¶€ í™•ì¸
    if NEW_DOCS:
        print(f"â• ì¶”ê°€í•  ìƒˆ ë¬¸ì„œ ìˆ˜: {len(NEW_DOCS)}")
    else:
        print("â„¹ï¸  ì¶”ê°€í•  ìƒˆ ë¬¸ì„œ ì—†ìŒ (ì •ë ¬ë§Œ ìˆ˜í–‰)")
    
    print("-" * 60)
    
    # í‚¤ì›Œë“œ ê°œìˆ˜ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì‚½ì… ìœ„ì¹˜ íŒŒì•…
    keyword_count_groups = {}
    for doc in existing_docs:
        count = len(doc.get("keywords", []))
        if count not in keyword_count_groups:
            keyword_count_groups[count] = []
        keyword_count_groups[count].append(doc)
    
    added_count = 0
    skipped_count = 0
    skipped_docs = []
    docs_to_add = []  # ì¶”ê°€í•  ë¬¸ì„œë“¤ì„ ëª¨ì•„ì„œ ë‚˜ì¤‘ì— ì •ë¦¬
    
    # ìƒˆ ë¬¸ì„œ ì¶”ê°€ (NEW_DOCSê°€ ìˆëŠ” ê²½ìš°ë§Œ)
    if NEW_DOCS:
        for new_doc in NEW_DOCS:
            is_dup, existing_id = is_duplicate(new_doc["text"], existing_docs)
            
            if is_dup:
                print(f"â­ï¸  ê±´ë„ˆëœ€ (ì¤‘ë³µ): ID {existing_id}ì™€ ìœ ì‚¬")
                print(f"   í…ìŠ¤íŠ¸: {new_doc['text'][:60]}...")
                skipped_count += 1
                skipped_docs.append(new_doc)
            else:
                # í‚¤ì›Œë“œ ì •ê·œí™” (ê¸°ì¡´ í˜•ì‹ì— ë§ì¶¤)
                normalized_keywords = []
                for kw in new_doc["keywords"]:
                    normalized_kw = normalize_keyword(kw, existing_docs)
                    normalized_keywords.append(normalized_kw)
                
                new_doc["keywords"] = normalized_keywords
                docs_to_add.append(new_doc)
                added_count += 1
    
    # ìƒˆ ë¬¸ì„œë¥¼ í‚¤ì›Œë“œ ê°œìˆ˜ë³„ ê·¸ë£¹ì— ì¶”ê°€ (NEW_DOCSê°€ ìˆëŠ” ê²½ìš°ë§Œ)
    if docs_to_add:
        new_docs_by_count = {}
        for doc in docs_to_add:
            count = len(doc.get("keywords", []))
            if count not in new_docs_by_count:
                new_docs_by_count[count] = []
            new_docs_by_count[count].append(doc)
        
        # ê° í‚¤ì›Œë“œ ê°œìˆ˜ ê·¸ë£¹ì— ìƒˆ ë¬¸ì„œ ì¶”ê°€
        for count in sorted(new_docs_by_count.keys()):
            if count not in keyword_count_groups:
                keyword_count_groups[count] = []
            
            # ê°™ì€ í‚¤ì›Œë“œ ê°œìˆ˜ ë‚´ì—ì„œ ì •ë ¬ (í‚¤ì›Œë“œ ì¢…ë¥˜ë³„)
            def sort_key(doc):
                keywords = sorted(doc.get("keywords", []))
                return tuple(keywords) + (doc.get("text", ""),)
            
            new_docs_by_count[count].sort(key=sort_key)
            
            keyword_count_groups[count].extend(new_docs_by_count[count])
            print(f"âœ… {count}ê°œ í‚¤ì›Œë“œ ë¬¸ì„œ {len(new_docs_by_count[count])}ê°œ ì¶”ê°€ ì˜ˆì •")
    
    # í‚¤ì›Œë“œ ê°œìˆ˜ë³„ë¡œ ì¬ì •ë ¬í•˜ê³  ID ì¬í• ë‹¹
    reorganized = []
    new_id = 1
    
    for keyword_count in sorted(keyword_count_groups.keys()):
        docs_in_group = keyword_count_groups[keyword_count]
        
        # ê°™ì€ í‚¤ì›Œë“œ ê°œìˆ˜ ë‚´ì—ì„œ ì •ë ¬ (í‚¤ì›Œë“œ ì¢…ë¥˜ë³„)
        def sort_key(doc):
            keywords = sorted(doc.get("keywords", []))
            return tuple(keywords) + (doc.get("text", ""),)
        
        docs_in_group.sort(key=sort_key)
        
        for doc in docs_in_group:
            doc["id"] = new_id
            reorganized.append(doc)
            new_id += 1
    
    # ì¶”ê°€ëœ ë¬¸ì„œ ì •ë³´ ì¶œë ¥ (NEW_DOCSê°€ ìˆëŠ” ê²½ìš°ë§Œ)
    if docs_to_add:
        print("-" * 60)
        for doc in docs_to_add:
            # ì¬ì •ë ¬ í›„ ID ì°¾ê¸°
            for reorg_doc in reorganized:
                if (reorg_doc.get("text") == doc.get("text") and 
                    reorg_doc.get("keywords") == doc.get("keywords")):
                    print(f"âœ… ì¶”ê°€ë¨: ID {reorg_doc['id']}, í‚¤ì›Œë“œ: {reorg_doc['keywords']}")
                    print(f"   í…ìŠ¤íŠ¸: {reorg_doc['text'][:60]}...")
                    break
    
    # ë°±ì—… íŒŒì¼ ê´€ë¦¬ (ì €ì¥ ì „ì— ë°±ì—…)
    BACKUP_FILE = JSON_FILE.with_suffix(".json.backup")
    if BACKUP_FILE.exists():
        backup2 = JSON_FILE.parent / "docs_ald.json.backup2"
        if backup2.exists():
            backup2.unlink()
        BACKUP_FILE.rename(backup2)
        print(f"ğŸ’¾ ê¸°ì¡´ ë°±ì—…ì„ backup2ë¡œ ì´ë™")
    
    # í˜„ì¬ íŒŒì¼ì„ ë°±ì—…ìœ¼ë¡œ ì €ì¥ (ì›ë³¸ ë°ì´í„°)
    with open(BACKUP_FILE, "w", encoding="utf-8") as f:
        json.dump(original_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ í˜„ì¬ íŒŒì¼ì„ ë°±ì—…ìœ¼ë¡œ ì €ì¥: {BACKUP_FILE}")
    
    # ê²°ê³¼ ì €ì¥
    data["documents"] = reorganized
    
    # ìƒˆ íŒŒì¼ ì €ì¥
    with open(JSON_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print("-" * 60)
    print(f"âœ… ì™„ë£Œ!")
    if removed_count > 0:
        print(f"   ê¸°ì¡´ ë¬¸ì„œ ì¤‘ë³µ ì œê±°: {removed_count}ê°œ")
    if NEW_DOCS:
        print(f"   ì¶”ê°€ëœ ë¬¸ì„œ: {added_count}ê°œ")
        print(f"   ê±´ë„ˆë›´ ë¬¸ì„œ: {skipped_count}ê°œ")
    print(f"   ì´ ë¬¸ì„œ ìˆ˜: {len(reorganized)}ê°œ")
    
    # í‚¤ì›Œë“œ ê°œìˆ˜ë³„ í†µê³„ ì¶œë ¥
    print(f"\nğŸ“‹ í‚¤ì›Œë“œ ê°œìˆ˜ë³„ ë¶„ë¥˜:")
    for count in sorted(keyword_count_groups.keys()):
        start_id = sum(len(keyword_count_groups[c]) for c in sorted(keyword_count_groups.keys()) if c < count) + 1
        end_id = start_id + len(keyword_count_groups[count]) - 1
        print(f"   {count}ê°œ í‚¤ì›Œë“œ: ID {start_id} ~ {end_id} ({len(keyword_count_groups[count])}ê°œ)")
    
    if skipped_docs:
        print(f"\nâš ï¸  ê±´ë„ˆë›´ ë¬¸ì„œ ëª©ë¡:")
        for doc in skipped_docs:
            print(f"   - {doc['text'][:60]}...")


if __name__ == "__main__":
    merge_documents()
