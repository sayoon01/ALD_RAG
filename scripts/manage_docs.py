# manage_docs.py
"""
docs_ald.json ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸

- stats : í‚¤ì›Œë“œë³„ ë¬¸ì¥ ê°œìˆ˜ í†µê³„ ì¶œë ¥
- group : í‚¤ì›Œë“œë³„ë¡œ ê·¸ë£¹í™”í•´ì„œ ë¬¸ì„œ ë³´ê¸°
- add   : í‚¤ì›Œë“œ + ë¬¸ì¥ ì¶”ê°€ (ì¸í„°ë™í‹°ë¸Œ OR ì˜µì…˜ ì…ë ¥)

ì‚¬ìš© ì˜ˆì‹œ:
    cd ~/ald-rag-lab
    python manage_docs.py stats
    python manage_docs.py group
    python manage_docs.py add
    python manage_docs.py add --keyword Precursor --text "ìƒˆë¡œ ì¶”ê°€í•  ë¬¸ì¥..."
"""

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List

BASE_DIR = Path(__file__).resolve().parent.parent  # ~/ald-rag-lab
DOCS_PATH = BASE_DIR / "docs" / "docs_ald.json"


# ==============================
# 1) íŒŒì¼ ìœ í‹¸
# ==============================

def load_raw_docs() -> List[Dict[str, Any]]:
    """
    docs_ald.jsonì„ ì½ì–´ì„œ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ë¥¼ ë°˜í™˜.
    - ìƒˆ í¬ë§·: { "documents": [{"id": 1, "keywords": ["ALD"], "text": "..."}, ...] }
    - ê¸°ì¡´ í¬ë§· í˜¸í™˜ ì§€ì›
    """
    if not DOCS_PATH.exists():
        print(f"[WARN] {DOCS_PATH} ê°€ ì•„ì§ ì—†ìŒ. ìƒˆë¡œ ìƒì„±í•  ì˜ˆì •.")
        return []

    with DOCS_PATH.open("r", encoding="utf-8") as f:
        data = json.load(f)

    if isinstance(data, dict) and "documents" in data:
        documents = data["documents"]
        
        # ìƒˆ êµ¬ì¡° (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
        if isinstance(documents, list):
            return documents
        
        # ê¸°ì¡´ êµ¬ì¡° (í‚¤ì›Œë“œë³„ ê·¸ë£¹í™”) - ìƒˆ êµ¬ì¡°ë¡œ ë³€í™˜
        elif isinstance(documents, dict):
            result = []
            next_id = 1
            for keyword, text_list in documents.items():
                if not isinstance(text_list, list):
                    continue
                for item in text_list:
                    if isinstance(item, dict):
                        text = str(item.get("text", "")).strip()
                    elif isinstance(item, str):
                        text = item.strip()
                    else:
                        continue
                    if not text:
                        continue
                    result.append({
                        "id": next_id,
                        "keywords": [keyword],
                        "text": text
                    })
                    next_id += 1
            return result

    print("[WARN] docs_ald.json êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦„. ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.")
    return []


def save_raw_docs(docs: List[Dict[str, Any]]) -> None:
    """
    ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ë¡œ ì €ì¥:
    {
      "documents": [
        {"id": 1, "keywords": ["ALD"], "text": "..."},
        ...
      ]
    }
    """
    DOCS_PATH.parent.mkdir(exist_ok=True, parents=True)
    wrapper = {"documents": docs}
    with DOCS_PATH.open("w", encoding="utf-8") as f:
        json.dump(wrapper, f, ensure_ascii=False, indent=2)
    print(f"[INFO] ì €ì¥ ì™„ë£Œ: {DOCS_PATH} (ì´ {len(docs)} ë¬¸ì„œ)")


def get_next_id(docs: List[Dict[str, Any]]) -> int:
    """ë‹¤ìŒ ID ë²ˆí˜¸ ê³„ì‚°"""
    max_id = 0
    for item in docs:
        try:
            item_id = int(item.get("id", 0))
            if item_id > max_id:
                max_id = item_id
        except (ValueError, TypeError):
            continue
    return max_id + 1


# ==============================
# 2) stats ëª¨ë“œ
# ==============================

def run_stats():
    if not DOCS_PATH.exists():
        print(f"[stats] {DOCS_PATH} ê°€ ì—†ìŒ.")
        return

    docs = load_raw_docs()
    if not docs:
        print("[stats] ë¬¸ì„œê°€ ë¹„ì–´ ìˆìŒ.")
        return

    # í‚¤ì›Œë“œë³„ ì¹´ìš´íŠ¸
    keyword_counts: Dict[str, int] = {}
    for item in docs:
        keywords = item.get("keywords", [])
        if isinstance(keywords, list):
            for kw in keywords:
                kw = str(kw).strip()
                if kw:
                    keyword_counts[kw] = keyword_counts.get(kw, 0) + 1
        elif isinstance(keywords, str):
            kw = keywords.strip()
            if kw:
                keyword_counts[kw] = keyword_counts.get(kw, 0) + 1

    print("\n[í‚¤ì›Œë“œ í†µê³„] (docs_ald.json ê¸°ì¤€)")
    for kw in sorted(keyword_counts.keys()):
        print(f"- {kw}: {keyword_counts[kw]} ë¬¸ì¥")


# ==============================
# 2-1) group ëª¨ë“œ: í‚¤ì›Œë“œë³„ë¡œ ê·¸ë£¹í™”í•´ì„œ ë³´ê¸°
# ==============================

def run_group():
    """
    í‚¤ì›Œë“œë³„ë¡œ ë¬¸ì„œë¥¼ ê·¸ë£¹í™”í•´ì„œ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
    """
    if not DOCS_PATH.exists():
        print(f"[group] {DOCS_PATH} ê°€ ì—†ìŒ.")
        return

    docs = load_raw_docs()
    if not docs:
        print("[group] ë¬¸ì„œê°€ ë¹„ì–´ ìˆìŒ.")
        return

    # í‚¤ì›Œë“œë³„ë¡œ ê·¸ë£¹í™”
    grouped: Dict[str, List[Dict[str, Any]]] = {}
    for item in docs:
        keywords = item.get("keywords", [])
        if isinstance(keywords, list):
            for kw in keywords:
                kw = str(kw).strip()
                if kw:
                    if kw not in grouped:
                        grouped[kw] = []
                    grouped[kw].append(item)
        elif isinstance(keywords, str):
            kw = keywords.strip()
            if kw:
                if kw not in grouped:
                    grouped[kw] = []
                grouped[kw].append(item)

    print("\n" + "=" * 80)
    print("[í‚¤ì›Œë“œë³„ ë¬¸ì„œ ê·¸ë£¹]")
    print("=" * 80)

    total_count = len(docs)
    for kw in sorted(grouped.keys()):
        items = grouped[kw]
        print(f"\nğŸ“Œ keyword = {kw} ({len(items)}ê°œ)")
        print("-" * 80)
        for idx, item in enumerate(items, 1):
            text = str(item.get("text", "")).strip()
            item_id = item.get("id", "?")
            keywords_str = ", ".join(item.get("keywords", [])) if isinstance(item.get("keywords"), list) else str(item.get("keywords", ""))
            print(f"  {idx}. [ID:{item_id}] [{keywords_str}] {text}")
    
    print("\n" + "=" * 80)
    print(f"ì´ {total_count}ê°œ ë¬¸ì„œ, {len(grouped)}ê°œ í‚¤ì›Œë“œ")
    print("=" * 80)


# ==============================
# 3) add ëª¨ë“œ
# ==============================

def run_add(keyword: str, text: str):
    docs = load_raw_docs()

    # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
    if not keyword:
        keyword = input("í‚¤ì›Œë“œ ì…ë ¥ (ì˜ˆ: Precursor, Purge, Plasma, MFC, Flow...): ").strip()
    if not text:
        text = input("ë¬¸ì¥ ì…ë ¥: ").strip()

    if not keyword or not text:
        print("[ERROR] í‚¤ì›Œë“œì™€ ë¬¸ì¥ì€ ë¹„ì–´ìˆìœ¼ë©´ ì•ˆ ë¨.")
        return

    # í‚¤ì›Œë“œë¥¼ ë°°ì—´ë¡œ ë³€í™˜ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ê²½ìš° ì²˜ë¦¬)
    keywords_list = [kw.strip() for kw in keyword.split(",") if kw.strip()]

    # ìƒˆ í•­ëª© ì¶”ê°€
    next_id = get_next_id(docs)
    new_item = {
        "id": next_id,
        "keywords": keywords_list,
        "text": text
    }
    docs.append(new_item)
    save_raw_docs(docs)

    print("\n[ì¶”ê°€ëœ í•­ëª©]")
    print(f"- id      : {next_id}")
    print(f"- keywords: {', '.join(keywords_list)}")
    print(f"- text    : {text}")


# ==============================
# 4) ë©”ì¸
# ==============================

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="docs_ald.json ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸")
    sub = parser.add_subparsers(dest="command", required=True)

    # stats
    sub.add_parser("stats", help="í‚¤ì›Œë“œë³„ ë¬¸ì¥ ê°œìˆ˜ í†µê³„ ì¶œë ¥")

    # group
    sub.add_parser("group", help="í‚¤ì›Œë“œë³„ë¡œ ê·¸ë£¹í™”í•´ì„œ ë¬¸ì„œ ë³´ê¸°")

    # add
    p_add = sub.add_parser("add", help="ë¬¸ì¥ ì¶”ê°€")
    p_add.add_argument("--keyword", type=str, default="", help="í‚¤ì›Œë“œ (ì—†ìœ¼ë©´ ì¸í„°ë™í‹°ë¸Œ)")
    p_add.add_argument("--text", type=str, default="", help="ë¬¸ì¥ ë‚´ìš© (ì—†ìœ¼ë©´ ì¸í„°ë™í‹°ë¸Œ)")

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    if args.command == "stats":
        run_stats()
    elif args.command == "group":
        run_group()
    elif args.command == "add":
        run_add(args.keyword, args.text)
