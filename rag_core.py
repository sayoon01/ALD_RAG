# rag_core.py

import json
import os
from pathlib import Path
from typing import List, Tuple, Dict, Any, Union, Optional

import numpy as np  # type: ignore
import torch  # type: ignore
from sentence_transformers import SentenceTransformer  # type: ignore
from transformers import AutoTokenizer, AutoModelForCausalLM  # type: ignore

# ==============================
# 0) ê²½ë¡œ / ìƒìˆ˜ ì„¤ì •
# ==============================

BASE_DIR = Path(__file__).resolve().parent           # ~/ald-rag-lab
DOCS_PATH = BASE_DIR / "docs" / "docs_ald.json"
SYNONYMS_PATH = BASE_DIR / "synonyms.json"
FEEDBACK_PATH = BASE_DIR / "feedback" / "feedback_data.json"

EMBED_MODEL_NAME = "thenlper/gte-small"
LLM_MODEL_NAME = "meta-llama/Meta-Llama-3-8B-Instruct"
# Fine-tuned ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©)
FINETUNED_MODEL_PATH = None  # ì˜ˆ: BASE_DIR / "finetuning" / "models" / "ald-llama-lora"

# ì „ì—­ ìƒíƒœ
DOCS: List[str] = []
DOC_KEYWORDS: List[str] = []
DOC_ITEMS: List[Dict[str, Any]] = []
DOC_EMBEDS: Optional[np.ndarray] = None

# í•´ì‹œ í…Œì´ë¸” ì¸ë±ìŠ¤ (í‚¤ì›Œë“œ -> ë¬¸ì„œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸)
KEYWORD_INDEX: Dict[str, List[int]] = {}  # keyword -> [doc_idx1, doc_idx2, ...]
KEYWORD_INDEX_NORMALIZED: Dict[str, List[int]] = {}  # normalized keyword -> [doc_idx1, doc_idx2, ...]

DEVICE: Optional[torch.device] = None
SEMB_MODEL: Optional[SentenceTransformer] = None
TOKENIZER: Optional[AutoTokenizer] = None
LLM: Optional[AutoModelForCausalLM] = None

MODEL_INFO: Dict[str, Any] = {}

# ë™ì˜ì–´ ë§¤í•‘
SYNONYM_GROUPS: List[List[str]] = []
SYNONYM_MAP: Dict[str, str] = {}  # keyword -> normalized keyword

# í”¼ë“œë°± ê¸°ë°˜ ì ìˆ˜ ì¡°ì • (ë¬¸ì„œ ì¸ë±ìŠ¤ -> ì ìˆ˜ ë³´ì •ê°’)
FEEDBACK_SCORES: Dict[int, float] = {}  # ë¬¸ì„œ ì¸ë±ìŠ¤ -> ì ìˆ˜ ë³´ì •ê°’ (ì–‘ìˆ˜: ë³´ë„ˆìŠ¤, ìŒìˆ˜: í˜ë„í‹°)
FEEDBACK_LAST_MODIFIED: float = 0.0  # í”¼ë“œë°± íŒŒì¼ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„ (ìºì‹±ìš©)
import os
FEEDBACK_LAST_MODIFIED: float = 0.0  # í”¼ë“œë°± íŒŒì¼ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„ (ìºì‹±ìš©)


# ==============================
# 1) JSON ë¬¸ì„œ ë¡œë”©
# ==============================

def load_docs(path: Path = DOCS_PATH):
    if not path.exists():
        raise FileNotFoundError(f"docs íŒŒì¼ì´ ì—†ìŒ: {path}")

    with path.open("r", encoding="utf-8") as f:
        raw = json.load(f)

    docs, keywords, pairs = [], [], []

    # ìƒˆ êµ¬ì¡°: {"documents": [{"id": 1, "keywords": ["ALD"], "text": "..."}, ...]}
    if isinstance(raw, dict) and "documents" in raw:
        documents = raw["documents"]
        
        if isinstance(documents, list):
            # ìƒˆ êµ¬ì¡°: keywordsê°€ ë°°ì—´ì¸ í˜•íƒœ
            for i, item in enumerate(documents):
                if not isinstance(item, dict):
                    print(f"[!] ê²½ê³ : ë¬¸ì„œ #{i} í˜•ì‹ ì˜¤ë¥˜. dict ì•„ë‹˜ â†’ ê±´ë„ˆëœ€")
                    continue

                text = str(item.get("text", "")).strip()
                item_keywords = item.get("keywords", [])
                
                # keywordsê°€ ë°°ì—´ì¸ ê²½ìš°
                if isinstance(item_keywords, list):
                    if not text or not item_keywords:
                        continue
                    
                    # ê° í‚¤ì›Œë“œë§ˆë‹¤ ë³„ë„ì˜ í•­ëª©ìœ¼ë¡œ ì¶”ê°€ (ê²€ìƒ‰ í˜¸í™˜ì„±)
                    for kw in item_keywords:
                        kw = str(kw).strip()
                        if not kw:
                            continue
                        docs.append(text)
                        keywords.append(kw)
                        pairs.append({"text": text, "keyword": kw, "id": item.get("id")})
                
                # ê¸°ì¡´ êµ¬ì¡° í˜¸í™˜: keywordê°€ ë‹¨ì¼ ë¬¸ìì—´ì¸ ê²½ìš°
                elif isinstance(item_keywords, str):
                    kw = item_keywords.strip()
                    if not text or not kw:
                        continue
                    docs.append(text)
                    keywords.append(kw)
                    pairs.append({"text": text, "keyword": kw, "id": item.get("id")})
                
                # ê¸°ì¡´ êµ¬ì¡° í˜¸í™˜: keyword í•„ë“œê°€ ìˆëŠ” ê²½ìš°
                elif "keyword" in item:
                    kw = str(item.get("keyword", "unknown")).strip()
                    if not text or not kw:
                        continue
                    docs.append(text)
                    keywords.append(kw)
                    pairs.append({"text": text, "keyword": kw, "id": item.get("id")})
        
        # ê¸°ì¡´ êµ¬ì¡° í˜¸í™˜: documentsê°€ dictì¸ ê²½ìš° (í‚¤ì›Œë“œë³„ ê·¸ë£¹í™”)
        elif isinstance(documents, dict):
            for keyword, text_list in documents.items():
                if not isinstance(text_list, list):
                    continue
                
                for item in text_list:
                    if isinstance(item, dict):
                        text = str(item.get("text", "")).strip()
                    elif isinstance(item, str):
                        text = item.strip()
                    else:
                        continue
                    
                    if not text:
                        continue
                    
                    docs.append(text)
                    keywords.append(keyword)
                    pairs.append({"text": text, "keyword": keyword})
        else:
            raise ValueError("docs_ald.json í˜•ì‹ ì˜¤ë¥˜ â€” documentsëŠ” list ë˜ëŠ” dictì—¬ì•¼ í•¨")
    else:
        raise ValueError("docs_ald.json í˜•ì‹ ì˜¤ë¥˜ â€” ë°˜ë“œì‹œ { 'documents': [...] } í˜•íƒœì—¬ì•¼ í•¨")

    print(f"[+] ë¬¸ì„œ ë¡œë”© ì™„ë£Œ â€” ì´ {len(docs)}ê°œ")
    return docs, keywords, pairs


# ==============================
# 2) ëª¨ë¸ ì´ˆê¸°í™”
# ==============================

def _build_keyword_index():
    """í‚¤ì›Œë“œ ê¸°ë°˜ í•´ì‹œ í…Œì´ë¸” ì¸ë±ìŠ¤ êµ¬ì¶•"""
    global KEYWORD_INDEX, KEYWORD_INDEX_NORMALIZED
    
    KEYWORD_INDEX = {}
    KEYWORD_INDEX_NORMALIZED = {}
    
    for idx, keyword in enumerate(DOC_KEYWORDS):
        # ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­
        if keyword not in KEYWORD_INDEX:
            KEYWORD_INDEX[keyword] = []
        KEYWORD_INDEX[keyword].append(idx)
        
        # ì •ê·œí™”ëœ í‚¤ì›Œë“œ ë§¤ì¹­ (ë™ì˜ì–´ ì§€ì›)
        normalized = normalize_keyword(keyword)
        if normalized not in KEYWORD_INDEX_NORMALIZED:
            KEYWORD_INDEX_NORMALIZED[normalized] = []
        KEYWORD_INDEX_NORMALIZED[normalized].append(idx)
    
    print(f"[+] í‚¤ì›Œë“œ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(KEYWORD_INDEX)}ê°œ í‚¤ì›Œë“œ")


def _init_models_if_needed():
    global DOCS, DOC_KEYWORDS, DOC_ITEMS, DOC_EMBEDS
    global SEMB_MODEL, TOKENIZER, LLM, DEVICE, DTYPE, MODEL_INFO
    global KEYWORD_INDEX, KEYWORD_INDEX_NORMALIZED

    if DOC_EMBEDS is not None:
        # ë¬¸ì„œëŠ” ì´ë¯¸ ë¡œë“œë˜ì—ˆì§€ë§Œ í”¼ë“œë°± ì ìˆ˜ëŠ” ë‹¤ì‹œ ë¡œë“œ (ìµœì‹  ìƒíƒœ ìœ ì§€)
        load_feedback_scores()
        return  # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìŒ

    # ë™ì˜ì–´ ë¡œë”©
    load_synonyms()
    
    # ë¬¸ì„œ ë¡œë”©
    DOCS, DOC_KEYWORDS, DOC_ITEMS = load_docs()
    
    # í‚¤ì›Œë“œ ì¸ë±ìŠ¤ êµ¬ì¶• (í•´ì‹œ í…Œì´ë¸”)
    _build_keyword_index()

    # ì„ë² ë”© ëª¨ë¸
    print(f"[+] Embedding model ë¡œë”© ì¤‘: {EMBED_MODEL_NAME}")
    SEMB_MODEL = SentenceTransformer(EMBED_MODEL_NAME)

    DOC_EMBEDS = SEMB_MODEL.encode(
        DOCS,
    normalize_embeddings=True,
    convert_to_numpy=True
).astype("float32")

    print("[+] ë¬¸ì„œ ì„ë² ë”© shape:", DOC_EMBEDS.shape)

    # ë””ë°”ì´ìŠ¤ ê²°ì •
    if torch.cuda.is_available():
        DEVICE = torch.device("cuda")
        DTYPE = torch.float16
    elif torch.backends.mps.is_available():
        DEVICE = torch.device("mps")
        DTYPE = torch.float16
    else:
        DEVICE = torch.device("cpu")
        DTYPE = torch.float32

    # LLaMA ë¡œë”©
    print(f"[+] LLaMA ë¡œë”© ì¤‘: {LLM_MODEL_NAME}")
    TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)

    LLM = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL_NAME,
        torch_dtype=DTYPE,
        device_map="auto" if DEVICE.type != "cpu" else None
    )

    # Fine-tuned ëª¨ë¸ ë¡œë“œ (LoRA adapter)
    if FINETUNED_MODEL_PATH is not None and Path(FINETUNED_MODEL_PATH).exists():
        try:
            try:
                from peft import PeftModel  # type: ignore
            except ImportError:
                print(f"[!] peft ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ Fine-tuned ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"[!] pip install peft ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
            else:
                print(f"[+] Fine-tuned adapter ë¡œë”© ì¤‘: {FINETUNED_MODEL_PATH}")
                LLM = PeftModel.from_pretrained(LLM, FINETUNED_MODEL_PATH)
                print(f"[+] Fine-tuned adapter ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print(f"[!] Fine-tuned ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"[!] ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    print(f"[+] LLaMA ë¡œë”© ì™„ë£Œ (device={DEVICE})")

    MODEL_INFO.update({
        "num_docs": len(DOCS),
        "keywords": sorted(list(set(DOC_KEYWORDS))),
        "embed_model": EMBED_MODEL_NAME,
        "llm_model": LLM_MODEL_NAME,
        "device": str(DEVICE),
    })


def reload_documents():
    """
    docs_ald.json íŒŒì¼ì„ ë‹¤ì‹œ ë¡œë“œí•˜ê³  ì„ë² ë”©ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤.
    ëª¨ë¸ì€ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤ (SEMB_MODELì´ ìˆì–´ì•¼ í•¨).
    """
    global DOCS, DOC_KEYWORDS, DOC_ITEMS, DOC_EMBEDS, MODEL_INFO, SEMB_MODEL
    
    if SEMB_MODEL is None:
        # ëª¨ë¸ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì „ì²´ ì´ˆê¸°í™”
        _init_models_if_needed()
        return
    
    print("[+] ë¬¸ì„œ ì¬ë¡œë”© ì¤‘...")
    
    # ë¬¸ì„œ ë‹¤ì‹œ ë¡œë”©
    DOCS, DOC_KEYWORDS, DOC_ITEMS = load_docs()
    
    # í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• (í•´ì‹œ í…Œì´ë¸”)
    _build_keyword_index()
    
    # ì„ë² ë”© ì¬ìƒì„±
    DOC_EMBEDS = SEMB_MODEL.encode(
        DOCS,
        normalize_embeddings=True,
        convert_to_numpy=True
    ).astype("float32")
    
    print(f"[+] ë¬¸ì„œ ì¬ë¡œë”© ì™„ë£Œ â€” ì´ {len(DOCS)}ê°œ")
    print(f"[+] ë¬¸ì„œ ì„ë² ë”© shape: {DOC_EMBEDS.shape}")
    
    # MODEL_INFO ì—…ë°ì´íŠ¸
    MODEL_INFO.update({
        "num_docs": len(DOCS),
        "keywords": sorted(list(set(DOC_KEYWORDS))),
        })
    
    return len(DOCS)


# ==============================
# 3) ë™ì˜ì–´ ë§¤í•‘ ë¡œë”©
# ==============================

def load_synonyms(path: Path = SYNONYMS_PATH):
    """ë™ì˜ì–´ ê·¸ë£¹ì„ ë¡œë“œí•˜ê³  ë§¤í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    global SYNONYM_GROUPS, SYNONYM_MAP
    
    if not path.exists():
        print(f"[!] ë™ì˜ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {path}. ê¸°ë³¸ ë™ì˜ì–´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        SYNONYM_GROUPS = []
        SYNONYM_MAP = {}
        return
    
    try:
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
        
        SYNONYM_GROUPS = data.get("synonym_groups", [])
        
        # ê° ê·¸ë£¹ì˜ ì²« ë²ˆì§¸ í‚¤ì›Œë“œë¥¼ ì •ê·œí™” í‚¤ì›Œë“œë¡œ ì‚¬ìš©
        SYNONYM_MAP = {}
        for group in SYNONYM_GROUPS:
            if not group:
                continue
            normalized = group[0].lower()
            for keyword in group:
                SYNONYM_MAP[keyword.lower()] = normalized
        
        print(f"[+] ë™ì˜ì–´ ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(SYNONYM_GROUPS)}ê°œ ê·¸ë£¹")
    except Exception as e:
        print(f"[!] ë™ì˜ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        SYNONYM_GROUPS = []
        SYNONYM_MAP = {}


def normalize_keyword(keyword: str) -> str:
    """í‚¤ì›Œë“œë¥¼ ì •ê·œí™”ëœ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if not keyword:
        return ""
    normalized = keyword.lower().strip()
    return SYNONYM_MAP.get(normalized, normalized)


def are_synonyms(keyword1: str, keyword2: str) -> bool:
    """ë‘ í‚¤ì›Œë“œê°€ ë™ì˜ì–´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    return normalize_keyword(keyword1) == normalize_keyword(keyword2)


# ==============================
# 3-2) í”¼ë“œë°± ê¸°ë°˜ ì ìˆ˜ ì¡°ì • ë¡œë”©
# ==============================

def load_feedback_scores(path: Path = FEEDBACK_PATH, force_reload: bool = False):
    """
    í”¼ë“œë°± ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ ë¬¸ì„œë³„ ì ìˆ˜ ì¡°ì • ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    ì¢‹ì€ í”¼ë“œë°±ì„ ë°›ì€ ë¬¸ì„œëŠ” ì ìˆ˜ ì¦ê°€, ë‚˜ìœ í”¼ë“œë°±ì€ ê°ì†Œ.
    
    Args:
        path: í”¼ë“œë°± íŒŒì¼ ê²½ë¡œ
        force_reload: Trueì´ë©´ ìºì‹œë¥¼ ë¬´ì‹œí•˜ê³  ê°•ì œë¡œ ë‹¤ì‹œ ë¡œë“œ
    """
    global FEEDBACK_SCORES, FEEDBACK_LAST_MODIFIED
    
    # DOCSê°€ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìŠ¤í‚µ
    if not DOCS:
        FEEDBACK_SCORES = {}
        return
    
    if not path.exists():
        FEEDBACK_SCORES = {}
        FEEDBACK_LAST_MODIFIED = 0.0
        return
    
    # ìºì‹±: íŒŒì¼ì´ ë³€ê²½ë˜ì§€ ì•Šì•˜ê³  ê°•ì œ ë¦¬ë¡œë“œê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ
    try:
        current_mtime = os.path.getmtime(path)
        if not force_reload and FEEDBACK_SCORES and current_mtime == FEEDBACK_LAST_MODIFIED:
            return  # ìºì‹œëœ ê°’ ì‚¬ìš©
        FEEDBACK_LAST_MODIFIED = current_mtime
    except OSError:
        FEEDBACK_SCORES = {}
        return
    
    try:
        with path.open("r", encoding="utf-8") as f:
            feedback_data = json.load(f)
        
        feedbacks = feedback_data.get("feedbacks", [])
        if not feedbacks:
            FEEDBACK_SCORES = {}
            return
        
        # ë¬¸ì„œ í…ìŠ¤íŠ¸ -> ì¸ë±ìŠ¤ ë§¤í•‘ (íš¨ìœ¨ì ì¸ ì¡°íšŒë¥¼ ìœ„í•´)
        doc_text_to_idx = {}
        for idx, doc_text in enumerate(DOCS):
            normalized_text = doc_text.strip()
            # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¡œ ë§¤í•‘ (ì¤‘ë³µ ë°©ì§€)
            if normalized_text not in doc_text_to_idx:
                doc_text_to_idx[normalized_text] = []
            doc_text_to_idx[normalized_text].append(idx)
        
        # ê° ë¬¸ì„œì— ëŒ€í•œ í”¼ë“œë°± ì ìˆ˜ ëˆ„ì 
        doc_feedback_scores = {}  # ì¸ë±ìŠ¤ -> ëˆ„ì  ì ìˆ˜
        
        for feedback in feedbacks:
            if feedback.get("feedback") not in ["like", "dislike"]:
                continue
            
            contexts = feedback.get("contexts", [])
            feedback_type = feedback.get("feedback")
            
            # ì¢‹ì€ í”¼ë“œë°±: +1, ë‚˜ìœ í”¼ë“œë°±: -1
            score_adjustment = 1.0 if feedback_type == "like" else -1.0
            
            # ê° ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´ í•´ë‹¹í•˜ëŠ” ë¬¸ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
            for ctx in contexts:
                ctx_text = str(ctx.get("text", "")).strip()
                if not ctx_text:
                    continue
                
                # íš¨ìœ¨ì ì¸ ë§¤ì¹­: ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¡œ ì§ì ‘ ì¡°íšŒ
                ctx_normalized = ctx_text
                
                # ì •í™•í•œ ë§¤ì¹­ ìš°ì„ 
                if ctx_normalized in doc_text_to_idx:
                    for idx in doc_text_to_idx[ctx_normalized]:
                        if idx not in doc_feedback_scores:
                            doc_feedback_scores[idx] = 0.0
                        doc_feedback_scores[idx] += score_adjustment
                else:
                    # ë¶€ë¶„ ë§¤ì¹­ (ì •í™•í•œ ë§¤ì¹­ì´ ì—†ëŠ” ê²½ìš°ë§Œ)
                    for doc_text, idx_list in doc_text_to_idx.items():
                        if ctx_normalized in doc_text or doc_text in ctx_normalized:
                            for idx in idx_list:
                                if idx not in doc_feedback_scores:
                                    doc_feedback_scores[idx] = 0.0
                                doc_feedback_scores[idx] += score_adjustment * 0.5  # ë¶€ë¶„ ë§¤ì¹­ì€ ê°€ì¤‘ì¹˜ 0.5
        
        # ì ìˆ˜ë¥¼ ì •ê·œí™” (ìµœëŒ€ Â±0.2 ë²”ìœ„ë¡œ ì œí•œ)
        if doc_feedback_scores:
            max_abs_score = max(abs(s) for s in doc_feedback_scores.values())
            if max_abs_score > 0:
                FEEDBACK_SCORES = {
                    idx: (score / max_abs_score) * 0.2  # ìµœëŒ€ Â±0.2 ë³´ì •
                    for idx, score in doc_feedback_scores.items()
                }
            else:
                FEEDBACK_SCORES = {}
        else:
            FEEDBACK_SCORES = {}
        
    except Exception as e:
        print(f"[!] í”¼ë“œë°± ì ìˆ˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
        FEEDBACK_SCORES = {}


# ==============================
# 4) í‚¤ì›Œë“œ í†µê³„
# ==============================

def get_keyword_stats() -> Dict[str, int]:
    _init_models_if_needed()
    stats: Dict[str, int] = {}
    for kw in DOC_KEYWORDS:
        stats[kw] = stats.get(kw, 0) + 1
    return stats


# ==============================
# 5) ê²€ìƒ‰ (Retrieval) - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
# ==============================

def extract_keywords_from_query(query: str) -> List[str]:
    """ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    query_lower = query.lower()
    found_keywords = []
    
    # ëª¨ë“  í‚¤ì›Œë“œ í™•ì¸
    for keyword in set(DOC_KEYWORDS):
        if keyword.lower() in query_lower:
            found_keywords.append(keyword)
    
    # ë™ì˜ì–´ë„ í™•ì¸
    for keyword in set(DOC_KEYWORDS):
        normalized_kw = normalize_keyword(keyword)
        # ì¿¼ë¦¬ì˜ ë‹¨ì–´ë“¤ì„ í™•ì¸
        query_words = query_lower.split()
        for word in query_words:
            if normalize_keyword(word) == normalized_kw:
                if keyword not in found_keywords:
                    found_keywords.append(keyword)
    
    return found_keywords


def compute_keyword_score(doc_keyword: str, query: str) -> float:
    """ë¬¸ì„œ í‚¤ì›Œë“œì™€ ì¿¼ë¦¬ ê°„ì˜ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    doc_keyword_lower = doc_keyword.lower()
    query_lower = query.lower()
    
    # 1. ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­
    if doc_keyword_lower in query_lower:
        return 1.0
    
    # 2. ë™ì˜ì–´ ë§¤ì¹­
    query_words = query_lower.split()
    for word in query_words:
        if are_synonyms(word, doc_keyword):
            return 0.8
    
    # 3. ë¶€ë¶„ ë§¤ì¹­ (í‚¤ì›Œë“œê°€ ì¿¼ë¦¬ì— í¬í•¨)
    if doc_keyword_lower in query_lower or query_lower in doc_keyword_lower:
        return 0.5
    
    return 0.0


def retrieve(
    query: str,
    top_k: int = 3,
    filter_keyword: Optional[str] = None,
    hybrid_weight: float = 0.3,  # í‚¤ì›Œë“œ ì ìˆ˜ ê°€ì¤‘ì¹˜ (0.0 = ì˜ë¯¸ ê²€ìƒ‰ë§Œ, 1.0 = í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ)
):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: í‚¤ì›Œë“œ ë§¤ì¹­ + ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (í•´ì‹œ í…Œì´ë¸” ìµœì í™”)
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        top_k: ìƒìœ„ Kê°œ ê²°ê³¼ ë°˜í™˜
        filter_keyword: í‚¤ì›Œë“œ í•„í„° (ì„ íƒ)
        hybrid_weight: í‚¤ì›Œë“œ ì ìˆ˜ ê°€ì¤‘ì¹˜ (0.0-1.0)
    """
    _init_models_if_needed()

    # 1. í•´ì‹œ í…Œì´ë¸”ì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í›„ë³´ ë¬¸ì„œ í•„í„°ë§ (O(1) ì¡°íšŒ)
    candidate_idxs = None
    
    # ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    query_keywords = extract_keywords_from_query(query)
    
    if query_keywords or filter_keyword:
        candidate_idxs_set = set()
        
        # ì¿¼ë¦¬ í‚¤ì›Œë“œë¡œ í›„ë³´ ë¬¸ì„œ ì°¾ê¸°
        for kw in query_keywords:
            # ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­
            if kw in KEYWORD_INDEX:
                candidate_idxs_set.update(KEYWORD_INDEX[kw])
            # ì •ê·œí™”ëœ í‚¤ì›Œë“œ ë§¤ì¹­
            normalized_kw = normalize_keyword(kw)
            if normalized_kw in KEYWORD_INDEX_NORMALIZED:
                candidate_idxs_set.update(KEYWORD_INDEX_NORMALIZED[normalized_kw])
        
        # í•„í„° í‚¤ì›Œë“œ ì ìš©
        if filter_keyword:
            filter_idxs_set = set()
            # ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­
            if filter_keyword in KEYWORD_INDEX:
                filter_idxs_set.update(KEYWORD_INDEX[filter_keyword])
            # ì •ê·œí™”ëœ í‚¤ì›Œë“œ ë§¤ì¹­
            filter_normalized = normalize_keyword(filter_keyword)
            if filter_normalized in KEYWORD_INDEX_NORMALIZED:
                filter_idxs_set.update(KEYWORD_INDEX_NORMALIZED[filter_normalized])
            
            if candidate_idxs_set:
                # êµì§‘í•©: ì¿¼ë¦¬ í‚¤ì›Œë“œì™€ í•„í„° í‚¤ì›Œë“œ ëª¨ë‘ ë§Œì¡±
                candidate_idxs_set = candidate_idxs_set & filter_idxs_set
            else:
                # í•„í„°ë§Œ ì ìš©
                candidate_idxs_set = filter_idxs_set
        
        if candidate_idxs_set:
            candidate_idxs = list(candidate_idxs_set)
    
    # í•„í„° í‚¤ì›Œë“œë§Œ ìˆê³  ì¿¼ë¦¬ í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°
    if filter_keyword and not query_keywords and candidate_idxs is None:
        candidate_idxs_set = set()
        if filter_keyword in KEYWORD_INDEX:
            candidate_idxs_set.update(KEYWORD_INDEX[filter_keyword])
        filter_normalized = normalize_keyword(filter_keyword)
        if filter_normalized in KEYWORD_INDEX_NORMALIZED:
            candidate_idxs_set.update(KEYWORD_INDEX_NORMALIZED[filter_normalized])
        if candidate_idxs_set:
            candidate_idxs = list(candidate_idxs_set)
    
    # í•´ë‹¹ keyword ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
    if (filter_keyword or query_keywords) and (candidate_idxs is None or len(candidate_idxs) == 0):
        return []
    
    # ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ ì¸ë±ìŠ¤ ê²°ì •
    search_idxs = candidate_idxs if candidate_idxs is not None else list(range(len(DOCS)))

    # 2. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (í›„ë³´ ë¬¸ì„œì— ëŒ€í•´ì„œë§Œ ìˆ˜í–‰)
    q_emb = SEMB_MODEL.encode(
        [query],
        normalize_embeddings=True,
        convert_to_numpy=True
    )[0].astype("float32")

    # í›„ë³´ ë¬¸ì„œì˜ ì„ë² ë”©ë§Œ ì‚¬ìš©
    candidate_embeds = DOC_EMBEDS[search_idxs]
    semantic_scores_candidate = np.dot(candidate_embeds, q_emb)
    
    # ì „ì²´ ë¬¸ì„œì— ëŒ€í•œ ì ìˆ˜ ë°°ì—´ ìƒì„±
    semantic_scores = np.full(len(DOCS), -1.0, dtype=np.float32)
    for i, idx in enumerate(search_idxs):
        semantic_scores[idx] = semantic_scores_candidate[i]
    
    # ì •ê·œí™”: 0-1 ë²”ìœ„ë¡œ ë³€í™˜ (í›„ë³´ ë¬¸ì„œë§Œ ê³ ë ¤)
    valid_scores = semantic_scores[search_idxs]
    semantic_min, semantic_max = valid_scores.min(), valid_scores.max()
    if semantic_max > semantic_min:
        semantic_scores_normalized = np.full(len(DOCS), 0.0, dtype=np.float32)
        for idx in search_idxs:
            semantic_scores_normalized[idx] = (semantic_scores[idx] - semantic_min) / (semantic_max - semantic_min)
    else:
        semantic_scores_normalized = np.full(len(DOCS), 0.5, dtype=np.float32)

    # 3. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (í›„ë³´ ë¬¸ì„œì— ëŒ€í•´ì„œë§Œ)
    keyword_scores = np.zeros(len(DOCS), dtype=np.float32)
    for idx in search_idxs:
        keyword_scores[idx] = compute_keyword_score(DOC_KEYWORDS[idx], query)

    # 4. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê²°í•©
    hybrid_scores = (1 - hybrid_weight) * semantic_scores_normalized + hybrid_weight * keyword_scores
    
    # 5. í”¼ë“œë°± ê¸°ë°˜ ì ìˆ˜ ì¡°ì • ì ìš©
    load_feedback_scores()  # ìµœì‹  í”¼ë“œë°± ë°˜ì˜
    for idx in search_idxs:
        if idx in FEEDBACK_SCORES:
            hybrid_scores[idx] += FEEDBACK_SCORES[idx]
            # ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
            hybrid_scores[idx] = max(0.0, min(1.0, hybrid_scores[idx]))

    # 6. ìƒìœ„ K ì„ íƒ (í›„ë³´ ë¬¸ì„œ ì¤‘ì—ì„œë§Œ)
    sorted_idx = sorted(search_idxs, key=lambda i: -hybrid_scores[i])[:top_k]

    return [
        (DOCS[i], float(hybrid_scores[i]), DOC_KEYWORDS[i])
        for i in sorted_idx
    ]


def debug_retrieval(query: str, retrieved):
    print("\n[ğŸ” ê²€ìƒ‰ ë””ë²„ê·¸]")
    print(f"- ì§ˆë¬¸: {query}")

    if not retrieved:
        print("  (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")
        return

    scores = [s for _, s, _ in retrieved]
    print(f"  * score range: min={min(scores):.3f}, max={max(scores):.3f}")

    for text, score, keyword in retrieved:
        print(f"    - [{keyword}] score={score:.3f} | {text}")


# ==============================
# 5) LLaMA ê¸°ë°˜ RAG ìƒì„±
# ==============================

def generate_answer(
    query: str,
    top_k: int = 3,
    max_new_tokens: int = 256,
    filter_keyword: Optional[str] = None,
    context_only: bool = False,
    debug: bool = False,
):

    _init_models_if_needed()
    
    # ëª¨ë¸ì´ ì•„ì§ ë¡œë”© ì¤‘ì¸ì§€ í™•ì¸
    if LLM is None or TOKENIZER is None:
        return (
            "ëª¨ë¸ì´ ì•„ì§ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            []
        )

    # ê²€ìƒ‰ ìˆ˜í–‰
    retrieved = retrieve(query, top_k=top_k, filter_keyword=filter_keyword)

    if debug:
        debug_retrieval(query, retrieved)

    if not retrieved:
        return (
            "í•´ë‹¹ ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ë§¥ì„ ì°¾ì§€ ëª»í–ˆì–´.\n"
            "â†’ filter_keywordê°€ ë„ˆë¬´ ì¢ê±°ë‚˜\n"
            "â†’ docs_ald.jsonì— ê´€ë ¨ ë¬¸ì¥ì´ ë¶€ì¡±í•  ìˆ˜ ìˆì–´.",
            []
        )

    scores = [s for _, s, _ in retrieved]
    max_score = max(scores)

    # ì•ˆì „ì¥ì¹˜
    if max_score < 0.45:  
        return (
            "ë¬¸ë§¥ê³¼ì˜ ì—°ê´€ì„±ì´ ë„ˆë¬´ ë‚®ì•„ì„œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ì•Šì•˜ì–´.\n"
            "ë¬¸ì„œë¥¼ ë³´ê°•í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë°”ê¿”ì¤˜!",
            retrieved
        )

    # contextë§Œ ë°˜í™˜ ëª¨ë“œ
    if context_only:
        return "ì»¨í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í–ˆì–´.", retrieved

    # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    ctx = "\n".join([f"- ({kw}) {text}" for text, _, kw in retrieved])

    system_prompt = (
        "ë°˜ë„ì²´ ALD ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ 'ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤'ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.\n"
        "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤."
    )

    user_prompt = f"""
ì•„ë˜ëŠ” ê´€ë ¨ ë¬¸ë§¥ì´ì•¼:

{ctx}

[ì§ˆë¬¸]
{query}

ìœ„ ë¬¸ë§¥ë§Œ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜.
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    full_prompt = TOKENIZER.apply_chat_template(
        messages, add_generation_prompt=True, tokenize=False
    )

    inputs = TOKENIZER(full_prompt, return_tensors="pt")
    
    # device_map="auto"ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ëª¨ë¸ì˜ ì‹¤ì œ ë””ë°”ì´ìŠ¤ë¥¼ í™•ì¸í•˜ê³ 
    # inputsë¥¼ ê·¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
    if hasattr(LLM, "device"):
        # ëª¨ë¸ì´ ë‹¨ì¼ ë””ë°”ì´ìŠ¤ì— ìˆëŠ” ê²½ìš°
        model_device = LLM.device
    elif hasattr(LLM, "hf_device_map"):
        # device_map="auto"ë¡œ ì—¬ëŸ¬ ë””ë°”ì´ìŠ¤ì— ë¶„ì‚°ëœ ê²½ìš°
        # ì²« ë²ˆì§¸ ë ˆì´ì–´ì˜ ë””ë°”ì´ìŠ¤ë¥¼ í™•ì¸
        first_param = next(LLM.parameters())
        model_device = first_param.device
    else:
        # ê¸°ë³¸ê°’ìœ¼ë¡œ DEVICE ì‚¬ìš©
        model_device = DEVICE if DEVICE is not None else torch.device("cpu")
    
    # inputsë¥¼ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    inputs = {k: v.to(model_device) for k, v in inputs.items()}

    with torch.no_grad():
        output_ids = LLM.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.6,
            top_p=0.9,
            do_sample=True,
            pad_token_id=TOKENIZER.eos_token_id
        )

    gen_ids = output_ids[0][inputs["input_ids"].shape[1]:]
    answer = TOKENIZER.decode(gen_ids, skip_special_tokens=True)

    return answer.strip(), retrieved
