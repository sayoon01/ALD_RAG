# rag_core.py

import json
from pathlib import Path
from typing import List, Tuple, Dict, Any

import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM

# ==============================
# 0) ê²½ë¡œ / ìƒìˆ˜ ì„¤ì •
# ==============================

BASE_DIR = Path(__file__).resolve().parent           # ~/ald-rag-lab
DOCS_PATH = BASE_DIR / "docs" / "docs_ald.json"

EMBED_MODEL_NAME = "thenlper/gte-small"
LLM_MODEL_NAME = "meta-llama/Meta-Llama-3-8B-Instruct"

# ì „ì—­ ìƒíƒœ
DOCS: List[str] = []
DOC_KEYWORDS: List[str] = []
DOC_ITEMS: List[Dict[str, Any]] = []
DOC_EMBEDS: np.ndarray | None = None

DEVICE: torch.device | None = None
DTYPE: torch.dtype | None = None
EMB_MODEL: SentenceTransformer | None = None
TOKENIZER: AutoTokenizer | None = None
LLM: AutoModelForCausalLM | None = None

MODEL_INFO: Dict[str, Any] = {}


# ==============================
# 1) JSON ë¬¸ì„œ ë¡œë”©
# ==============================

def load_docs(path: Path = DOCS_PATH):
    if not path.exists():
        raise FileNotFoundError(f"docs íŒŒì¼ì´ ì—†ìŒ: {path}")

    with path.open("r", encoding="utf-8") as f:
        raw = json.load(f)

    # JSON í˜•íƒœ: {"documents": [ {...}, ... ]}
    if isinstance(raw, dict) and "documents" in raw:
        items = raw["documents"]
    else:
        raise ValueError("docs_ald.json í˜•ì‹ ì˜¤ë¥˜ â€” ë°˜ë“œì‹œ { 'documents': [...] } í˜•íƒœì—¬ì•¼ í•¨")

    docs, keywords, pairs = [], [], []

    for i, item in enumerate(items):
        if not isinstance(item, dict):
            print(f"[!] ê²½ê³ : ë¬¸ì„œ #{i} í˜•ì‹ ì˜¤ë¥˜. dict ì•„ë‹˜ â†’ ê±´ë„ˆëœ€")
            continue

        text = str(item.get("text", "")).strip()
        kw = str(item.get("keyword", "unknown")).strip()

        if not text:
            print(f"[!] ê²½ê³ : ë¬¸ì„œ #{i} text ë¹„ì–´ìˆìŒ â†’ ê±´ë„ˆëœ€")
            continue

        docs.append(text)
        keywords.append(kw)
        pairs.append({"text": text, "keyword": kw})

    print(f"[+] ë¬¸ì„œ ë¡œë”© ì™„ë£Œ â€” ì´ {len(docs)}ê°œ")
    return docs, keywords, pairs


# ==============================
# 2) ëª¨ë¸ ì´ˆê¸°í™”
# ==============================

def _init_models_if_needed():
    global DOCS, DOC_KEYWORDS, DOC_ITEMS, DOC_EMBEDS
    global EMB_MODEL, TOKENIZER, LLM, DEVICE, DTYPE, MODEL_INFO

    if DOC_EMBEDS is not None:
        return  # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìŒ

    # ë¬¸ì„œ ë¡œë”©
    DOCS, DOC_KEYWORDS, DOC_ITEMS = load_docs()

    # ì„ë² ë”© ëª¨ë¸
    print(f"[+] Embedding model ë¡œë”© ì¤‘: {EMBED_MODEL_NAME}")
    EMB_MODEL = SentenceTransformer(EMBED_MODEL_NAME)

    DOC_EMBEDS = EMB_MODEL.encode(
        DOCS,
        normalize_embeddings=True,
        convert_to_numpy=True
    ).astype("float32")

    print("[+] ë¬¸ì„œ ì„ë² ë”© shape:", DOC_EMBEDS.shape)

    # ë””ë°”ì´ìŠ¤ ê²°ì •
    if torch.cuda.is_available():
        DEVICE = torch.device("cuda")
        DTYPE = torch.float16
    elif torch.backends.mps.is_available():
        DEVICE = torch.device("mps")
        DTYPE = torch.float16
    else:
        DEVICE = torch.device("cpu")
        DTYPE = torch.float32

    # LLaMA ë¡œë”©
    print(f"[+] LLaMA ë¡œë”© ì¤‘: {LLM_MODEL_NAME}")
    TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)

    LLM = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL_NAME,
        torch_dtype=DTYPE,
        device_map="auto" if DEVICE.type != "cpu" else None
    )

    print(f"[+] LLaMA ë¡œë”© ì™„ë£Œ (device={DEVICE})")

    MODEL_INFO.update({
        "num_docs": len(DOCS),
        "keywords": sorted(list(set(DOC_KEYWORDS))),
        "embed_model": EMBED_MODEL_NAME,
        "llm_model": LLM_MODEL_NAME,
        "device": str(DEVICE),
    })


def reload_documents():
    """
    docs_ald.json íŒŒì¼ì„ ë‹¤ì‹œ ë¡œë“œí•˜ê³  ì„ë² ë”©ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤.
    ëª¨ë¸ì€ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤ (EMB_MODELì´ ìˆì–´ì•¼ í•¨).
    """
    global DOCS, DOC_KEYWORDS, DOC_ITEMS, DOC_EMBEDS, MODEL_INFO
    
    if EMB_MODEL is None:
        # ëª¨ë¸ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì „ì²´ ì´ˆê¸°í™”
        _init_models_if_needed()
        return
    
    print("[+] ë¬¸ì„œ ì¬ë¡œë”© ì¤‘...")
    
    # ë¬¸ì„œ ë‹¤ì‹œ ë¡œë”©
    DOCS, DOC_KEYWORDS, DOC_ITEMS = load_docs()
    
    # ì„ë² ë”© ì¬ìƒì„±
    DOC_EMBEDS = EMB_MODEL.encode(
        DOCS,
        normalize_embeddings=True,
        convert_to_numpy=True
    ).astype("float32")
    
    print(f"[+] ë¬¸ì„œ ì¬ë¡œë”© ì™„ë£Œ â€” ì´ {len(DOCS)}ê°œ")
    print(f"[+] ë¬¸ì„œ ì„ë² ë”© shape: {DOC_EMBEDS.shape}")
    
    # MODEL_INFO ì—…ë°ì´íŠ¸
    MODEL_INFO.update({
        "num_docs": len(DOCS),
        "keywords": sorted(list(set(DOC_KEYWORDS))),
    })
    
    return len(DOCS)


# ==============================
# 3) í‚¤ì›Œë“œ í†µê³„
# ==============================

def get_keyword_stats() -> Dict[str, int]:
    _init_models_if_needed()
    stats: Dict[str, int] = {}
    for kw in DOC_KEYWORDS:
        stats[kw] = stats.get(kw, 0) + 1
    return stats


# ==============================
# 4) ê²€ìƒ‰ (Retrieval)
# ==============================

def retrieve(
    query: str,
    top_k: int = 3,
    filter_keyword: str | None = None,
):
    _init_models_if_needed()

    q_emb = EMB_MODEL.encode(
        [query],
        normalize_embeddings=True,
        convert_to_numpy=True
    )[0].astype("float32")

    scores = np.dot(DOC_EMBEDS, q_emb)

    # í‚¤ì›Œë“œ í•„í„° ì ìš©
    idxs = range(len(DOCS))

    if filter_keyword:
        idxs = [i for i in idxs if DOC_KEYWORDS[i].lower() == filter_keyword.lower()]

    # í•´ë‹¹ keyword ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
    if filter_keyword and not idxs:
        return []

    # ìƒìœ„ K ì„ íƒ
    sorted_idx = sorted(idxs, key=lambda i: -scores[i])[:top_k]

    return [
        (DOCS[i], float(scores[i]), DOC_KEYWORDS[i])
        for i in sorted_idx
    ]


def debug_retrieval(query: str, retrieved):
    print("\n[ğŸ” ê²€ìƒ‰ ë””ë²„ê·¸]")
    print(f"- ì§ˆë¬¸: {query}")

    if not retrieved:
        print("  (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")
        return

    scores = [s for _, s, _ in retrieved]
    print(f"  * score range: min={min(scores):.3f}, max={max(scores):.3f}")

    for text, score, keyword in retrieved:
        print(f"    - [{keyword}] score={score:.3f} | {text}")


# ==============================
# 5) LLaMA ê¸°ë°˜ RAG ìƒì„±
# ==============================

def generate_answer(
    query: str,
    top_k: int = 3,
    max_new_tokens: int = 256,
    filter_keyword: str | None = None,
    context_only: bool = False,
    debug: bool = False,
):

    _init_models_if_needed()

    # ê²€ìƒ‰ ìˆ˜í–‰
    retrieved = retrieve(query, top_k=top_k, filter_keyword=filter_keyword)

    if debug:
        debug_retrieval(query, retrieved)

    if not retrieved:
        return (
            "í•´ë‹¹ ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ë§¥ì„ ì°¾ì§€ ëª»í–ˆì–´.\n"
            "â†’ filter_keywordê°€ ë„ˆë¬´ ì¢ê±°ë‚˜\n"
            "â†’ docs_ald.jsonì— ê´€ë ¨ ë¬¸ì¥ì´ ë¶€ì¡±í•  ìˆ˜ ìˆì–´.",
            []
        )

    scores = [s for _, s, _ in retrieved]
    max_score = max(scores)

    # ì•ˆì „ì¥ì¹˜
    if max_score < 0.45:  
        return (
            "ë¬¸ë§¥ê³¼ì˜ ì—°ê´€ì„±ì´ ë„ˆë¬´ ë‚®ì•„ì„œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ì•Šì•˜ì–´.\n"
            "ë¬¸ì„œë¥¼ ë³´ê°•í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë°”ê¿”ì¤˜!",
            retrieved
        )

    # contextë§Œ ë°˜í™˜ ëª¨ë“œ
    if context_only:
        return "ì»¨í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í–ˆì–´.", retrieved

    # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    ctx = "\n".join([f"- ({kw}) {text}" for text, _, kw in retrieved])

    system_prompt = (
        "ë„ˆëŠ” ë°˜ë„ì²´ ALD, í”Œë¼ì¦ˆë§ˆ, ìœ ëŸ‰, ì••ë ¥, ì±”ë²„ ê°œë…ì„ ì„¤ëª…í•˜ëŠ” ì¡°ìˆ˜ì•¼.\n"
        "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µí•´ì•¼ í•´.\n"
        "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ì¸¡í•˜ì§€ ë§ˆ.\n"
        "ë§Œì•½ ì •ë³´ê°€ ì—†ìœ¼ë©´ 'í•´ë‹¹ ë…¸íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.'ë¼ê³ ë§Œ ë§í•´."
    )

    user_prompt = f"""
ì•„ë˜ëŠ” ê´€ë ¨ ë¬¸ë§¥ì´ì•¼:

{ctx}

[ì§ˆë¬¸]
{query}

ìœ„ ë¬¸ë§¥ë§Œ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜.
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    full_prompt = TOKENIZER.apply_chat_template(
        messages, add_generation_prompt=True, tokenize=False
    )

    inputs = TOKENIZER(full_prompt, return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        output_ids = LLM.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.6,
            top_p=0.9,
            do_sample=True,
            pad_token_id=TOKENIZER.eos_token_id
        )

    gen_ids = output_ids[0][inputs["input_ids"].shape[1]:]
    answer = TOKENIZER.decode(gen_ids, skip_special_tokens=True)

    return answer.strip(), retrieved
