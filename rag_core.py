# rag_core.py

import json
from pathlib import Path
from typing import List, Tuple, Dict, Any, Union, Optional

import numpy as np  # type: ignore
import torch  # type: ignore
from sentence_transformers import SentenceTransformer  # type: ignore
from transformers import AutoTokenizer, AutoModelForCausalLM  # type: ignore

# ==============================
# 0) ê²½ë¡œ / ìƒìˆ˜ ì„¤ì •
# ==============================

BASE_DIR = Path(__file__).resolve().parent           # ~/ald-rag-lab
DOCS_PATH = BASE_DIR / "docs" / "docs_ald.json"

EMBED_MODEL_NAME = "thenlper/gte-small"
LLM_MODEL_NAME = "meta-llama/Meta-Llama-3-8B-Instruct"

# ì „ì—­ ìƒíƒœ
DOCS: List[str] = []
DOC_KEYWORDS: List[str] = []
DOC_ITEMS: List[Dict[str, Any]] = []
DOC_EMBEDS: Optional[np.ndarray] = None

DEVICE: Optional[torch.device] = None
SEMB_MODEL: Optional[SentenceTransformer] = None
TOKENIZER: Optional[AutoTokenizer] = None
LLM: Optional[AutoModelForCausalLM] = None

MODEL_INFO: Dict[str, Any] = {}


# ==============================
# 1) JSON ë¬¸ì„œ ë¡œë”©
# ==============================

def load_docs(path: Path = DOCS_PATH):
    if not path.exists():
        raise FileNotFoundError(f"docs íŒŒì¼ì´ ì—†ìŒ: {path}")

    with path.open("r", encoding="utf-8") as f:
        raw = json.load(f)

    docs, keywords, pairs = [], [], []

    # ìƒˆ êµ¬ì¡°: {"documents": [{"id": 1, "keywords": ["ALD"], "text": "..."}, ...]}
    if isinstance(raw, dict) and "documents" in raw:
        documents = raw["documents"]
        
        if isinstance(documents, list):
            # ìƒˆ êµ¬ì¡°: keywordsê°€ ë°°ì—´ì¸ í˜•íƒœ
            for i, item in enumerate(documents):
                if not isinstance(item, dict):
                    print(f"[!] ê²½ê³ : ë¬¸ì„œ #{i} í˜•ì‹ ì˜¤ë¥˜. dict ì•„ë‹˜ â†’ ê±´ë„ˆëœ€")
                    continue

                text = str(item.get("text", "")).strip()
                item_keywords = item.get("keywords", [])
                
                # keywordsê°€ ë°°ì—´ì¸ ê²½ìš°
                if isinstance(item_keywords, list):
                    if not text or not item_keywords:
                        continue
                    
                    # ê° í‚¤ì›Œë“œë§ˆë‹¤ ë³„ë„ì˜ í•­ëª©ìœ¼ë¡œ ì¶”ê°€ (ê²€ìƒ‰ í˜¸í™˜ì„±)
                    for kw in item_keywords:
                        kw = str(kw).strip()
                        if not kw:
                            continue
                        docs.append(text)
                        keywords.append(kw)
                        pairs.append({"text": text, "keyword": kw, "id": item.get("id")})
                
                # ê¸°ì¡´ êµ¬ì¡° í˜¸í™˜: keywordê°€ ë‹¨ì¼ ë¬¸ìì—´ì¸ ê²½ìš°
                elif isinstance(item_keywords, str):
                    kw = item_keywords.strip()
                    if not text or not kw:
                        continue
                    docs.append(text)
                    keywords.append(kw)
                    pairs.append({"text": text, "keyword": kw, "id": item.get("id")})
                
                # ê¸°ì¡´ êµ¬ì¡° í˜¸í™˜: keyword í•„ë“œê°€ ìˆëŠ” ê²½ìš°
                elif "keyword" in item:
                    kw = str(item.get("keyword", "unknown")).strip()
                    if not text or not kw:
                        continue
                    docs.append(text)
                    keywords.append(kw)
                    pairs.append({"text": text, "keyword": kw, "id": item.get("id")})
        
        # ê¸°ì¡´ êµ¬ì¡° í˜¸í™˜: documentsê°€ dictì¸ ê²½ìš° (í‚¤ì›Œë“œë³„ ê·¸ë£¹í™”)
        elif isinstance(documents, dict):
            for keyword, text_list in documents.items():
                if not isinstance(text_list, list):
                    continue
                
                for item in text_list:
                    if isinstance(item, dict):
                        text = str(item.get("text", "")).strip()
                    elif isinstance(item, str):
                        text = item.strip()
                    else:
                        continue
                    
                    if not text:
                        continue
                    
                    docs.append(text)
                    keywords.append(keyword)
                    pairs.append({"text": text, "keyword": keyword})
        else:
            raise ValueError("docs_ald.json í˜•ì‹ ì˜¤ë¥˜ â€” documentsëŠ” list ë˜ëŠ” dictì—¬ì•¼ í•¨")
    else:
        raise ValueError("docs_ald.json í˜•ì‹ ì˜¤ë¥˜ â€” ë°˜ë“œì‹œ { 'documents': [...] } í˜•íƒœì—¬ì•¼ í•¨")

    print(f"[+] ë¬¸ì„œ ë¡œë”© ì™„ë£Œ â€” ì´ {len(docs)}ê°œ")
    return docs, keywords, pairs


# ==============================
# 2) ëª¨ë¸ ì´ˆê¸°í™”
# ==============================

def _init_models_if_needed():
    global DOCS, DOC_KEYWORDS, DOC_ITEMS, DOC_EMBEDS
    global EMB_MODEL, TOKENIZER, LLM, DEVICE, DTYPE, MODEL_INFO

    if DOC_EMBEDS is not None:
        return  # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìŒ

    # ë¬¸ì„œ ë¡œë”©
    DOCS, DOC_KEYWORDS, DOC_ITEMS = load_docs()

    # ì„ë² ë”© ëª¨ë¸
    print(f"[+] Embedding model ë¡œë”© ì¤‘: {EMBED_MODEL_NAME}")
    EMB_MODEL = SentenceTransformer(EMBED_MODEL_NAME)

    DOC_EMBEDS = EMB_MODEL.encode(
        DOCS,
        normalize_embeddings=True,
        convert_to_numpy=True
    ).astype("float32")

    print("[+] ë¬¸ì„œ ì„ë² ë”© shape:", DOC_EMBEDS.shape)

    # ë””ë°”ì´ìŠ¤ ê²°ì •
    if torch.cuda.is_available():
        DEVICE = torch.device("cuda")
        DTYPE = torch.float16
    elif torch.backends.mps.is_available():
        DEVICE = torch.device("mps")
        DTYPE = torch.float16
    else:
        DEVICE = torch.device("cpu")
        DTYPE = torch.float32

    # LLaMA ë¡œë”©
    print(f"[+] LLaMA ë¡œë”© ì¤‘: {LLM_MODEL_NAME}")
    TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)

    LLM = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL_NAME,
        torch_dtype=DTYPE,
        device_map="auto" if DEVICE.type != "cpu" else None
    )

    print(f"[+] LLaMA ë¡œë”© ì™„ë£Œ (device={DEVICE})")

    MODEL_INFO.update({
        "num_docs": len(DOCS),
        "keywords": sorted(list(set(DOC_KEYWORDS))),
        "embed_model": EMBED_MODEL_NAME,
        "llm_model": LLM_MODEL_NAME,
        "device": str(DEVICE),
    })


def reload_documents():
    """
    docs_ald.json íŒŒì¼ì„ ë‹¤ì‹œ ë¡œë“œí•˜ê³  ì„ë² ë”©ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤.
    ëª¨ë¸ì€ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤ (EMB_MODELì´ ìˆì–´ì•¼ í•¨).
    """
    global DOCS, DOC_KEYWORDS, DOC_ITEMS, DOC_EMBEDS, MODEL_INFO
    
    if EMB_MODEL is None:
        # ëª¨ë¸ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì „ì²´ ì´ˆê¸°í™”
        _init_models_if_needed()
        return
    
    print("[+] ë¬¸ì„œ ì¬ë¡œë”© ì¤‘...")
    
    # ë¬¸ì„œ ë‹¤ì‹œ ë¡œë”©
    DOCS, DOC_KEYWORDS, DOC_ITEMS = load_docs()
    
    # ì„ë² ë”© ì¬ìƒì„±
    DOC_EMBEDS = EMB_MODEL.encode(
        DOCS,
        normalize_embeddings=True,
        convert_to_numpy=True
    ).astype("float32")
    
    print(f"[+] ë¬¸ì„œ ì¬ë¡œë”© ì™„ë£Œ â€” ì´ {len(DOCS)}ê°œ")
    print(f"[+] ë¬¸ì„œ ì„ë² ë”© shape: {DOC_EMBEDS.shape}")
    
    # MODEL_INFO ì—…ë°ì´íŠ¸
    MODEL_INFO.update({
        "num_docs": len(DOCS),
        "keywords": sorted(list(set(DOC_KEYWORDS))),
    })
    
    return len(DOCS)


# ==============================
# 3) í‚¤ì›Œë“œ í†µê³„
# ==============================

def get_keyword_stats() -> Dict[str, int]:
    _init_models_if_needed()
    stats: Dict[str, int] = {}
    for kw in DOC_KEYWORDS:
        stats[kw] = stats.get(kw, 0) + 1
    return stats


# ==============================
# 4) ê²€ìƒ‰ (Retrieval)
# ==============================

def retrieve(
    query: str,
    top_k: int = 3,
    filter_keyword: Optional[str] = None,
):
    _init_models_if_needed()

    q_emb = EMB_MODEL.encode(
        [query],
        normalize_embeddings=True,
        convert_to_numpy=True
    )[0].astype("float32")

    scores = np.dot(DOC_EMBEDS, q_emb)

    # í‚¤ì›Œë“œ í•„í„° ì ìš©
    idxs = range(len(DOCS))

    if filter_keyword:
        idxs = [i for i in idxs if DOC_KEYWORDS[i].lower() == filter_keyword.lower()]

    # í•´ë‹¹ keyword ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
    if filter_keyword and not idxs:
        return []

    # ìƒìœ„ K ì„ íƒ
    sorted_idx = sorted(idxs, key=lambda i: -scores[i])[:top_k]

    return [
        (DOCS[i], float(scores[i]), DOC_KEYWORDS[i])
        for i in sorted_idx
    ]


def debug_retrieval(query: str, retrieved):
    print("\n[ğŸ” ê²€ìƒ‰ ë””ë²„ê·¸]")
    print(f"- ì§ˆë¬¸: {query}")

    if not retrieved:
        print("  (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")
        return

    scores = [s for _, s, _ in retrieved]
    print(f"  * score range: min={min(scores):.3f}, max={max(scores):.3f}")

    for text, score, keyword in retrieved:
        print(f"    - [{keyword}] score={score:.3f} | {text}")


# ==============================
# 5) LLaMA ê¸°ë°˜ RAG ìƒì„±
# ==============================

def generate_answer(
    query: str,
    top_k: int = 3,
    max_new_tokens: int = 256,
    filter_keyword: Optional[str] = None,
    context_only: bool = False,
    debug: bool = False,
):

    _init_models_if_needed()
    
    # ëª¨ë¸ì´ ì•„ì§ ë¡œë”© ì¤‘ì¸ì§€ í™•ì¸
    if LLM is None or TOKENIZER is None:
        return (
            "ëª¨ë¸ì´ ì•„ì§ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            []
        )

    # ê²€ìƒ‰ ìˆ˜í–‰
    retrieved = retrieve(query, top_k=top_k, filter_keyword=filter_keyword)

    if debug:
        debug_retrieval(query, retrieved)

    if not retrieved:
        return (
            "í•´ë‹¹ ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ë§¥ì„ ì°¾ì§€ ëª»í–ˆì–´.\n"
            "â†’ filter_keywordê°€ ë„ˆë¬´ ì¢ê±°ë‚˜\n"
            "â†’ docs_ald.jsonì— ê´€ë ¨ ë¬¸ì¥ì´ ë¶€ì¡±í•  ìˆ˜ ìˆì–´.",
            []
        )

    scores = [s for _, s, _ in retrieved]
    max_score = max(scores)

    # ì•ˆì „ì¥ì¹˜
    if max_score < 0.45:  
        return (
            "ë¬¸ë§¥ê³¼ì˜ ì—°ê´€ì„±ì´ ë„ˆë¬´ ë‚®ì•„ì„œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ì•Šì•˜ì–´.\n"
            "ë¬¸ì„œë¥¼ ë³´ê°•í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë°”ê¿”ì¤˜!",
            retrieved
        )

    # contextë§Œ ë°˜í™˜ ëª¨ë“œ
    if context_only:
        return "ì»¨í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í–ˆì–´.", retrieved

    # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    ctx = "\n".join([f"- ({kw}) {text}" for text, _, kw in retrieved])

    system_prompt = (
        "ë„ˆëŠ” ë°˜ë„ì²´ ALD, í”Œë¼ì¦ˆë§ˆ, ìœ ëŸ‰, ì••ë ¥, ì±”ë²„ ë“± ALD ê°œë…ì„ ì„¤ëª…í•˜ëŠ” ì¡°ìˆ˜ì•¼.\n"
        "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µí•´ì•¼ í•´.\n"
        "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ì¸¡í•˜ì§€ ë§ˆ.\n"
        "ë§Œì•½ ì •ë³´ê°€ ì—†ìœ¼ë©´ 'í•´ë‹¹ ë…¸íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.'ë¼ê³ ë§Œ ë§í•´."
    )

    user_prompt = f"""
ì•„ë˜ëŠ” ê´€ë ¨ ë¬¸ë§¥ì´ì•¼:

{ctx}

[ì§ˆë¬¸]
{query}

ìœ„ ë¬¸ë§¥ë§Œ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜.
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    full_prompt = TOKENIZER.apply_chat_template(
        messages, add_generation_prompt=True, tokenize=False
    )

    inputs = TOKENIZER(full_prompt, return_tensors="pt")
    
    # device_map="auto"ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ëª¨ë¸ì˜ ì‹¤ì œ ë””ë°”ì´ìŠ¤ë¥¼ í™•ì¸í•˜ê³ 
    # inputsë¥¼ ê·¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
    if hasattr(LLM, "device"):
        # ëª¨ë¸ì´ ë‹¨ì¼ ë””ë°”ì´ìŠ¤ì— ìˆëŠ” ê²½ìš°
        model_device = LLM.device
    elif hasattr(LLM, "hf_device_map"):
        # device_map="auto"ë¡œ ì—¬ëŸ¬ ë””ë°”ì´ìŠ¤ì— ë¶„ì‚°ëœ ê²½ìš°
        # ì²« ë²ˆì§¸ ë ˆì´ì–´ì˜ ë””ë°”ì´ìŠ¤ë¥¼ í™•ì¸
        first_param = next(LLM.parameters())
        model_device = first_param.device
    else:
        # ê¸°ë³¸ê°’ìœ¼ë¡œ DEVICE ì‚¬ìš©
        model_device = DEVICE if DEVICE is not None else torch.device("cpu")
    
    # inputsë¥¼ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    inputs = {k: v.to(model_device) for k, v in inputs.items()}

    with torch.no_grad():
        output_ids = LLM.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.6,
            top_p=0.9,
            do_sample=True,
            pad_token_id=TOKENIZER.eos_token_id
        )

    gen_ids = output_ids[0][inputs["input_ids"].shape[1]:]
    answer = TOKENIZER.decode(gen_ids, skip_special_tokens=True)

    return answer.strip(), retrieved
