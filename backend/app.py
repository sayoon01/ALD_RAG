# backend/app.py

import sys
from pathlib import Path
from typing import List, Optional

# ============================================
# 0) ìƒìœ„ í´ë”ë¥¼ import ê²½ë¡œì— ì¶”ê°€
# ============================================

BASE_DIR = Path(__file__).resolve().parent.parent  # ~/ald-rag-lab
FEEDBACK_PATH = BASE_DIR / "feedback" / "feedback_data.json"

if str(BASE_DIR) not in sys.path:
    sys.path.append(str(BASE_DIR))

# rag_core í•¨ìˆ˜ë“¤ ë¡œë”©
from rag_core import generate_answer, get_keyword_stats, reload_documents, load_feedback_scores

from fastapi import FastAPI, UploadFile, File, Form  # type: ignore
from fastapi.middleware.cors import CORSMiddleware  # type: ignore
from pydantic import BaseModel  # type: ignore
import json
from datetime import datetime
from typing import Optional
import uuid
from datetime import datetime
from typing import Optional


# ============================================
# FastAPI ê¸°ë³¸ ì„¸íŒ…
# ============================================

app = FastAPI(
    title="ë°˜ë„ì²´ ALD RAG API",
    description="ë°˜ë„ì²´ ì§€ì‹ ê¸°ë°˜ RAG ì±—ë´‡ API (LLaMA ê¸°ë°˜)",
    version="1.3.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================
# Request / Response ëª¨ë¸
# ============================================

class ChatRequest(BaseModel):
    question: str
    top_k: int = 3
    max_new_tokens: int = 256
    filter_keyword: Optional[str] = None
    context_only: bool = False
    debug: bool = False


class ContextItem(BaseModel):
    text: str
    score: float
    keyword: str


class ChatResponse(BaseModel):
    answer: str
    contexts: List[ContextItem]
    used_keyword: str
    session_id: Optional[str] = None  # í”¼ë“œë°± ì¶”ì ìš© ì„¸ì…˜ ID
    confidence: Optional[float] = None  # ë‹µë³€ ì‹ ë¢°ë„ ì ìˆ˜ (0.0 ~ 1.0)
    related_questions: Optional[List[str]] = None  # ê´€ë ¨ ì§ˆë¬¸ ëª©ë¡


class FeedbackRequest(BaseModel):
    session_id: str
    question: str
    answer: str
    contexts: List[ContextItem]
    feedback: str  # "like" or "dislike"
    comment: Optional[str] = None


# ============================================
# ê¸°ë³¸ ê±´ê°• ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
# ============================================

@app.get("/")
def root():
    try:
        # ëª¨ë¸ ì´ˆê¸°í™” (ë¬¸ì„œ ë¡œë”©)
        from rag_core import _init_models_if_needed, MODEL_INFO
        _init_models_if_needed()
        
        keywords = get_keyword_stats()
        num_docs = MODEL_INFO.get("num_docs", 0)
        keyword_list = MODEL_INFO.get("keywords", [])
    except Exception as e:
        keywords = {}
        num_docs = 0
        keyword_list = []
        print(f"[WARN] get_keyword_stats() ì‹¤íŒ¨: {e}")
    
    return {
        "message": "ALD RAG API ì •ìƒ ë™ì‘ ì¤‘",
        "keywords": keywords,
        "num_docs": num_docs,
        "keyword_list": keyword_list,
        "device": MODEL_INFO.get("device", "unknown"),
        "note": "POST /chat ìœ¼ë¡œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "endpoints": {
            "chat": "POST /chat - ì§ˆë¬¸í•˜ê¸°",
            "keywords": "GET /keywords - í‚¤ì›Œë“œ í†µê³„",
            "docs": "GET /docs - Swagger UI"
        }
    }


# ============================================
# í•µì‹¬: /chat RAG ì—”ë“œí¬ì¸íŠ¸
# ============================================

@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    """
    RAG ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸
    
    - question: ì§ˆë¬¸ ë‚´ìš© (í•„ìˆ˜)
    - top_k: ê²€ìƒ‰í•  ìƒìœ„ ë¬¸ì¥ ê°œìˆ˜ (ê¸°ë³¸: 3)
    - max_new_tokens: LLMì´ ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 256)
    - filter_keyword: íŠ¹ì • í‚¤ì›Œë“œë§Œ ê²€ìƒ‰ (ì„ íƒ)
    - context_only: ë‹µë³€ ìƒì„± ì—†ì´ ì»¨í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ (ê¸°ë³¸: False)
    - debug: ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥ (ê¸°ë³¸: False)
    """
    try:
        # generate_answer í˜¸ì¶œ (ëª¨ë“  íŒŒë¼ë¯¸í„° ì „ë‹¬)
        # ë°˜í™˜ê°’: (answer, retrieved, confidence, related_questions)
        result = generate_answer(
            query=req.question,
            top_k=req.top_k,
            max_new_tokens=req.max_new_tokens,
            filter_keyword=req.filter_keyword,
            context_only=req.context_only,
            debug=req.debug
        )
        
        # ë°˜í™˜ê°’ ì²˜ë¦¬ (í˜¸í™˜ì„± ìœ ì§€)
        if len(result) == 4:
            answer, retrieved, confidence, related_questions = result
        elif len(result) == 3:
            answer, retrieved, confidence = result
            related_questions = []
        elif len(result) == 2:
            answer, retrieved = result
            confidence = 0.0
            related_questions = []
        else:
            answer = str(result[0]) if result else "ì˜¤ë¥˜ ë°œìƒ"
            retrieved = []
            confidence = 0.0
            related_questions = []
    except Exception as e:
        # ëª¨ë¸ ì—ëŸ¬ / íŒŒì¼ ëˆ„ë½ ë“± ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´
        import traceback
        error_detail = traceback.format_exc()
        print(f"[ERROR] generate_answer ì‹¤íŒ¨:\n{error_detail}")
        
        return ChatResponse(
            answer=f"ì„œë²„ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\nìì„¸í•œ ë‚´ìš©ì€ ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            contexts=[],
            used_keyword=req.filter_keyword or ""
        )

    # retrieved ë³´í˜¸ â€” Noneì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
    retrieved = retrieved or []

    # ContextItem ë³€í™˜ (rag_coreëŠ” (text, score, keyword, doc_id) tuple ë°˜í™˜)
    context_items: List[ContextItem] = []
    for item in retrieved:
        try:
            if isinstance(item, tuple):
                if len(item) >= 4:
                    # ìƒˆ í˜•ì‹: (text, score, keyword, doc_id)
                    text, score, keyword, doc_id = item[0], item[1], item[2], item[3]
                elif len(item) >= 3:
                    # ê¸°ì¡´ í˜•ì‹: (text, score, keyword)
                    text, score, keyword = item[0], item[1], item[2]
                else:
                    text = str(item[0]) if len(item) > 0 else ""
                    score = float(item[1]) if len(item) > 1 else 0.0
                    keyword = ""
            elif isinstance(item, dict):
                # dict í˜•íƒœë„ ì§€ì› (ì•ˆì „ì¥ì¹˜)
                text = item.get("text", "")
                score = float(item.get("score", 0.0))
                keyword = item.get("keyword", "")
            else:
                # ì˜ˆìƒì¹˜ ëª»í•œ í˜•íƒœ
                print(f"[WARN] ì˜ˆìƒì¹˜ ëª»í•œ retrieved item í˜•íƒœ: {type(item)}")
                text = str(item)
                score = 0.0
                keyword = ""

            context_items.append(ContextItem(
                text=str(text),
                score=float(score),
                keyword=str(keyword) if keyword else ""
            ))
        except Exception as e:
            print(f"[WARN] ContextItem ë³€í™˜ ì‹¤íŒ¨: {e}, item={item}")
            continue

    # ì‹¤ì œ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ê²°ì • (filter_keywordê°€ ìˆìœ¼ë©´ ê·¸ê²ƒ, ì—†ìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ)
    used_keyword = req.filter_keyword or ""
    if not used_keyword and context_items:
        # ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚œ í‚¤ì›Œë“œ ì‚¬ìš©
        keyword_counts = {}
        for ctx in context_items:
            if ctx.keyword:
                keyword_counts[ctx.keyword] = keyword_counts.get(ctx.keyword, 0) + 1
        if keyword_counts:
            used_keyword = max(keyword_counts.items(), key=lambda x: x[1])[0]

    import uuid
    session_id = str(uuid.uuid4())[:8]  # ì§§ì€ ì„¸ì…˜ ID ìƒì„±
    
    return ChatResponse(
        answer=answer or "ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
        contexts=context_items,
        used_keyword=used_keyword,
        session_id=session_id,
        confidence=confidence if 'confidence' in locals() else None,
        related_questions=related_questions if 'related_questions' in locals() else []
    )


# ============================================
# í”¼ë“œë°± ìˆ˜ì§‘ ì—”ë“œí¬ì¸íŠ¸
# ============================================

@app.post("/feedback")
def submit_feedback(req: FeedbackRequest):
    """
    ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ (ğŸ‘/ğŸ‘)
    """
    try:
        # í”¼ë“œë°± ë°ì´í„° ë¡œë“œ
        if FEEDBACK_PATH.exists():
            with FEEDBACK_PATH.open("r", encoding="utf-8") as f:
                feedback_data = json.load(f)
        else:
            feedback_data = {"feedbacks": []}
        
        # ìƒˆ í”¼ë“œë°± ì¶”ê°€
        feedback_entry = {
            "session_id": req.session_id,
            "question": req.question,
            "answer": req.answer,
            "contexts": [{"text": ctx.text, "score": ctx.score, "keyword": ctx.keyword} for ctx in req.contexts],
            "feedback": req.feedback,  # "like" or "dislike"
            "comment": req.comment,
            "timestamp": datetime.now().isoformat()
        }
        
        feedback_data["feedbacks"].append(feedback_entry)
        
        # íŒŒì¼ì— ì €ì¥
        with FEEDBACK_PATH.open("w", encoding="utf-8") as f:
            json.dump(feedback_data, f, ensure_ascii=False, indent=2)
        
        # í”¼ë“œë°± ê¸°ë°˜ ì ìˆ˜ ì¡°ì • ê°±ì‹  (ê°•ì œ ë¦¬ë¡œë“œ)
        from rag_core import load_feedback_scores
        load_feedback_scores(force_reload=True)
        
        print(f"[+] í”¼ë“œë°± ìˆ˜ì§‘: {req.feedback} (session_id: {req.session_id})")
        print(f"[+] í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆê³ , ê²€ìƒ‰ ì ìˆ˜ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return {
            "success": True,
            "message": "í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆê³ , ë‹¤ìŒ ê²€ìƒ‰ë¶€í„° ë°˜ì˜ë©ë‹ˆë‹¤.",
            "session_id": req.session_id
        }
    except Exception as e:
        import traceback
        print(f"[ERROR] í”¼ë“œë°± ì €ì¥ ì‹¤íŒ¨: {traceback.format_exc()}")
        return {
            "success": False,
            "error": str(e)
        }


@app.get("/feedback/stats")
def get_feedback_stats():
    """í”¼ë“œë°± í†µê³„ ì¡°íšŒ"""
    try:
        if not FEEDBACK_PATH.exists():
            return {
                "total": 0,
                "likes": 0,
                "dislikes": 0,
                "recent_feedbacks": []
            }
        
        with FEEDBACK_PATH.open("r", encoding="utf-8") as f:
            feedback_data = json.load(f)
        
        feedbacks = feedback_data.get("feedbacks", [])
        likes = sum(1 for fb in feedbacks if fb.get("feedback") == "like")
        dislikes = sum(1 for fb in feedbacks if fb.get("feedback") == "dislike")
        
        # ìµœê·¼ 10ê°œ í”¼ë“œë°±
        recent = sorted(feedbacks, key=lambda x: x.get("timestamp", ""), reverse=True)[:10]
        
        return {
            "total": len(feedbacks),
            "likes": likes,
            "dislikes": dislikes,
            "recent_feedbacks": recent
        }
    except Exception as e:
        return {
            "error": str(e)
        }


# ============================================
# ì „ì²´ í‚¤ì›Œë“œ ëª©ë¡ ì œê³µ (í”„ë¡ íŠ¸ì—ì„œ Select ë°•ìŠ¤ ìš©)
# ============================================

@app.get("/keywords")
def keywords():
    """
    í‚¤ì›Œë“œë³„ ë¬¸ì„œ ê°œìˆ˜ í†µê³„ ë°˜í™˜
    
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í‚¤ì›Œë“œ í•„í„° ì„ íƒ ë°•ìŠ¤ë¥¼ ì±„ìš°ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    try:
        stats = get_keyword_stats()
        return stats if stats else {}
    except Exception as e:
        print(f"[ERROR] get_keyword_stats() ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}


# ============================================
# ë¬¸ì„œ ì¬ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸ (docs_ald.json ë³€ê²½ í›„ ì‚¬ìš©)
# ============================================

@app.post("/reload-docs")
def reload_docs():
    """
    docs_ald.json íŒŒì¼ì„ ë‹¤ì‹œ ë¡œë“œí•˜ê³  ì„ë² ë”©ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤.
    
    docs_ald.json íŒŒì¼ì„ ìˆ˜ì •í•œ í›„ ì´ ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•˜ë©´
    ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì§€ ì•Šê³ ë„ ìƒˆë¡œìš´ ë¬¸ì„œê°€ ê²€ìƒ‰ì— ë°˜ì˜ë©ë‹ˆë‹¤.
    """
    try:
        num_docs = reload_documents()
        stats = get_keyword_stats()
        return {
            "success": True,
            "message": f"ë¬¸ì„œ ì¬ë¡œë”© ì™„ë£Œ: {num_docs}ê°œ ë¬¸ì„œ",
            "num_docs": num_docs,
            "keywords": stats
        }
    except Exception as e:
        print(f"[ERROR] reload_documents() ì‹¤íŒ¨: {e}")
        import traceback
        error_detail = traceback.format_exc()
        print(error_detail)
        return {
            "success": False,
            "error": str(e),
            "detail": error_detail
        }


# ============================================
# ë¬¸ì„œ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸ë“¤
# ============================================

@app.get("/docs/stats")
def docs_stats():
    """í‚¤ì›Œë“œë³„ ë¬¸ì„œ ê°œìˆ˜ í†µê³„"""
    try:
        # ì§ì ‘ JSON íŒŒì¼ ì½ê¸° (ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ)
        docs_path = BASE_DIR / "docs" / "docs_ald.json"
        with open(docs_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        docs = data.get("documents", [])
        if not isinstance(docs, list):
            docs = []
        
        keyword_counts = {}
        
        for item in docs:
            if not isinstance(item, dict):
                continue
            keywords = item.get("keywords", [])
            if isinstance(keywords, list):
                for kw in keywords:
                    kw = str(kw).strip()
                    if kw:
                        keyword_counts[kw] = keyword_counts.get(kw, 0) + 1
            elif isinstance(keywords, str):
                kw = keywords.strip()
                if kw:
                    keyword_counts[kw] = keyword_counts.get(kw, 0) + 1
        
        return {
            "success": True,
            "stats": keyword_counts,
            "total_docs": len(docs)
        }
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": str(e),
            "detail": traceback.format_exc()
        }


@app.get("/docs/group")
def docs_group():
    """í‚¤ì›Œë“œë³„ë¡œ ê·¸ë£¹í™”ëœ ë¬¸ì„œ ëª©ë¡"""
    try:
        # ì§ì ‘ JSON íŒŒì¼ ì½ê¸° (ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ)
        docs_path = BASE_DIR / "docs" / "docs_ald.json"
        with open(docs_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        docs = data.get("documents", [])
        if not isinstance(docs, list):
            docs = []
        
        grouped = {}
        
        for item in docs:
            if not isinstance(item, dict):
                continue
            keywords = item.get("keywords", [])
            if isinstance(keywords, list):
                for kw in keywords:
                    kw = str(kw).strip()
                    if kw:
                        if kw not in grouped:
                            grouped[kw] = []
                        grouped[kw].append({
                            "id": item.get("id"),
                            "text": item.get("text", ""),
                            "keywords": item.get("keywords", [])
                        })
            elif isinstance(keywords, str):
                kw = keywords.strip()
                if kw:
                    if kw not in grouped:
                        grouped[kw] = []
                    grouped[kw].append({
                        "id": item.get("id"),
                        "text": item.get("text", ""),
                        "keywords": [keywords]
                    })
        
        return {
            "success": True,
            "grouped": grouped,
            "total_docs": len(docs),
            "total_keywords": len(grouped)
        }
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": str(e),
            "detail": traceback.format_exc()
        }


@app.post("/docs/add")
def docs_add(keyword: str = Form(...), text: str = Form(...)):
    """ìƒˆ ë¬¸ì„œ ì¶”ê°€"""
    try:
        # ì§ì ‘ JSON íŒŒì¼ ì½ê¸°
        docs_path = BASE_DIR / "docs" / "docs_ald.json"
        with open(docs_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        docs = data.get("documents", [])
        if not isinstance(docs, list):
            docs = []
        
        keywords_list = [kw.strip() for kw in keyword.split(",") if kw.strip()]
        
        # ë‹¤ìŒ ID ê³„ì‚°
        next_id = 1
        if docs:
            max_id = max((item.get("id", 0) for item in docs if isinstance(item, dict)), default=0)
            next_id = max_id + 1
        
        new_item = {
            "id": next_id,
            "keywords": keywords_list,
            "text": text
        }
        docs.append(new_item)
        
        # íŒŒì¼ ì €ì¥
        data["documents"] = docs
        with open(docs_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # ë¬¸ì„œ ì¬ë¡œë“œ
        reload_documents()
        
        return {
            "success": True,
            "message": "ë¬¸ì„œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤",
            "item": new_item
        }
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": str(e),
            "detail": traceback.format_exc()
        }


@app.post("/docs/extract")
async def docs_extract(
    file: UploadFile = File(...),
    keywords: str = Form(...),
    file_type: str = Form("text")
):
    """í…ìŠ¤íŠ¸/PDF íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ"""
    try:
        sys.path.insert(0, str(BASE_DIR))
        from scripts.doc_management.extract_from_docs import extract_from_text, extract_from_pdf, filter_quality_sentences
        from pathlib import Path
        import tempfile
        
        keywords_list = [kw.strip() for kw in keywords.split(",") if kw.strip()]
        
        # íŒŒì¼ ì½ê¸°
        content = await file.read()
        
        if file_type == "pdf":
            # PDF ì²˜ë¦¬
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(content)
                tmp_path = Path(tmp.name)
            
            try:
                results = extract_from_pdf(tmp_path, keywords_list)
            finally:
                tmp_path.unlink()
        else:
            # í…ìŠ¤íŠ¸ ì²˜ë¦¬
            text = content.decode("utf-8")
            results = extract_from_text(text, keywords_list)
        
        # í’ˆì§ˆ í•„í„°ë§
        for keyword in keywords_list:
            results[keyword] = filter_quality_sentences(results.get(keyword, []))
        
        # ì¤‘ë³µ ì œê±° ë° ë¬¸ì„œ ì¶”ê°€
        docs_path = BASE_DIR / "docs" / "docs_ald.json"
        with open(docs_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        docs = data.get("documents", [])
        if not isinstance(docs, list):
            docs = []
        
        existing_texts = {item.get("text", "") for item in docs if isinstance(item, dict)}
        new_count = 0
        
        # ë‹¤ìŒ ID ê³„ì‚°
        next_id = 1
        if docs:
            max_id = max((item.get("id", 0) for item in docs if isinstance(item, dict)), default=0)
            next_id = max_id + 1
        
        extracted_items = []
        for keyword in keywords_list:
            sentences = results.get(keyword, [])
            for sentence in sentences:
                if sentence not in existing_texts:
                    new_item = {
                        "id": next_id,
                        "keywords": [keyword],
                        "text": sentence
                    }
                    docs.append(new_item)
                    extracted_items.append(new_item)
                    existing_texts.add(sentence)
                    next_id += 1
                    new_count += 1
        
        if new_count > 0:
            # íŒŒì¼ ì €ì¥
            data["documents"] = docs
            with open(docs_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            reload_documents()
        
        return {
            "success": True,
            "message": f"{new_count}ê°œ ë¬¸ì¥ì´ ì¶”ì¶œë˜ì–´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤",
            "extracted": {kw: len(results.get(kw, [])) for kw in keywords_list},
            "added": new_count,
            "items": extracted_items[:10]  # ìµœëŒ€ 10ê°œë§Œ ë°˜í™˜
        }
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": str(e),
            "detail": traceback.format_exc()
        }


@app.post("/docs/generate")
def docs_generate(
    mode: str = Form(...),
    keyword: str = Form(...),
    count: int = Form(3)
):
    """LLM ë˜ëŠ” í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ìƒì„±"""
    try:
        sys.path.insert(0, str(BASE_DIR))
        from scripts.doc_management.generate_docs import run_llm_mode, run_template_mode
        
        if mode == "llm":
            # LLM ëª¨ë“œ (ê°„ë‹¨í•œ ë²„ì „ - ì‹¤ì œë¡œëŠ” generate_with_llm í˜¸ì¶œ)
            from rag_core import _init_models_if_needed, TOKENIZER, LLM, DEVICE
            import torch  # type: ignore
            
            _init_models_if_needed()
            
            if LLM is None or TOKENIZER is None:
                return {
                    "success": False,
                    "error": "LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
                }
            
            # ì§ì ‘ JSON íŒŒì¼ ì½ê¸°
            docs_path = BASE_DIR / "docs" / "docs_ald.json"
            with open(docs_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            docs = data.get("documents", [])
            if not isinstance(docs, list):
                docs = []
            
            existing_texts = []
            for item in docs:
                if not isinstance(item, dict):
                    continue
                keywords = item.get("keywords", [])
                if isinstance(keywords, list) and keyword in keywords:
                    existing_texts.append(item.get("text", ""))
                elif isinstance(keywords, str) and keywords == keyword:
                    existing_texts.append(item.get("text", ""))
            
            if not existing_texts:
                return {
                    "success": False,
                    "error": f"'{keyword}' í‚¤ì›Œë“œì— ê¸°ì¡´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
                }
            
            # LLMìœ¼ë¡œ ìƒì„± (ê°„ë‹¨í•œ ë²„ì „)
            context_examples = "\n".join([f"- {text}" for text in existing_texts[:3]])
            system_prompt = (
                "ë°˜ë„ì²´ ALD ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•˜ì„¸ìš”.\n"
                "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ 'ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤'ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.\n"
                "ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ì§§ê³  ëª…í™•í•œ ì„¤ëª… ë¬¸ì¥ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.\n"
                "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì˜ì–´ ì „ë¬¸ ìš©ì–´ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ë©ë‹ˆë‹¤.\n"
                "ê° ë¬¸ì¥ì€ ë…ë¦½ì ì´ê³ , ì‹¤ì œ ê³µì •ì—ì„œ ì‚¬ìš©ë˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ë‹´ì•„ì•¼ í•©ë‹ˆë‹¤."
            )
            user_prompt = f"""
í‚¤ì›Œë“œ: {keyword}

ê¸°ì¡´ ì˜ˆì‹œ ë¬¸ì¥ë“¤:
{context_examples}

ìœ„ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìƒˆë¡œìš´ ì„¤ëª… ë¬¸ì¥ì„ {count}ê°œ ìƒì„±í•´ì¤˜.
ê° ë¬¸ì¥ì€:
- 50ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ
- ì‹¤ì œ ALD ê³µì •ì—ì„œ ì‚¬ìš©ë˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´
- ê¸°ì¡´ ì˜ˆì‹œì™€ ìœ ì‚¬í•œ ìŠ¤íƒ€ì¼

ìƒì„±ëœ ë¬¸ì¥ë§Œ í•œ ì¤„ì— í•˜ë‚˜ì”© ì¶œë ¥í•´ì¤˜ (ë²ˆí˜¸ë‚˜ ì„¤ëª… ì—†ì´).
"""
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            full_prompt = TOKENIZER.apply_chat_template(
                messages, add_generation_prompt=True, tokenize=False
            )
            
            inputs = TOKENIZER(full_prompt, return_tensors="pt")
            
            # ë””ë°”ì´ìŠ¤ ì²˜ë¦¬
            if hasattr(LLM, "device"):
                model_device = LLM.device
            elif hasattr(LLM, "hf_device_map"):
                first_param = next(LLM.parameters())
                model_device = first_param.device
            else:
                model_device = DEVICE if DEVICE is not None else torch.device("cpu")
            
            inputs = {k: v.to(model_device) for k, v in inputs.items()}
            
            with torch.no_grad():
                output_ids = LLM.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0.8,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=TOKENIZER.eos_token_id
                )
            
            gen_ids = output_ids[0][inputs["input_ids"].shape[1]:]
            answer = TOKENIZER.decode(gen_ids, skip_special_tokens=True)
            
            lines = [line.strip() for line in answer.strip().split("\n") if line.strip()]
            cleaned_lines = []
            for line in lines:
                for prefix in ["1.", "2.", "3.", "4.", "5.", "- ", "* ", "â€¢ "]:
                    if line.startswith(prefix):
                        line = line[len(prefix):].strip()
                if line and len(line) > 10:
                    cleaned_lines.append(line)
            
            new_texts = cleaned_lines[:count]
            
            if not new_texts:
                return {
                    "success": False,
                    "error": "ìƒì„±ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"
                }
            
            # ë¬¸ì„œì— ì¶”ê°€
            # ë‹¤ìŒ ID ê³„ì‚°
            next_id = 1
            if docs:
                max_id = max((item.get("id", 0) for item in docs if isinstance(item, dict)), default=0)
                next_id = max_id + 1
            
            new_items = []
            for text in new_texts:
                new_item = {
                    "id": next_id,
                    "keywords": [keyword],
                    "text": text
                }
                docs.append(new_item)
                new_items.append(new_item)
                next_id += 1
            
            # íŒŒì¼ ì €ì¥
            data["documents"] = docs
            with open(docs_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            reload_documents()
            
            return {
                "success": True,
                "message": f"{len(new_texts)}ê°œ ë¬¸ì¥ì´ ìƒì„±ë˜ì–´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤",
                "items": new_items,
                "warning": "âš ï¸ ìƒì„±ëœ ë¬¸ì¥ì€ ë°˜ë“œì‹œ ì „ë¬¸ê°€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤"
            }
            
        elif mode == "template":
            from scripts.doc_management.generate_docs import generate_from_template
            
            new_texts = generate_from_template(keyword, count)
            
            if not new_texts:
                return {
                    "success": False,
                    "error": f"'{keyword}'ì— ëŒ€í•œ í…œí”Œë¦¿ì´ ì—†ìŠµë‹ˆë‹¤"
                }
            
            # ì§ì ‘ JSON íŒŒì¼ ì½ê¸°
            docs_path = BASE_DIR / "docs" / "docs_ald.json"
            with open(docs_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            docs = data.get("documents", [])
            if not isinstance(docs, list):
                docs = []
            
            # ë‹¤ìŒ ID ê³„ì‚°
            next_id = 1
            if docs:
                max_id = max((item.get("id", 0) for item in docs if isinstance(item, dict)), default=0)
                next_id = max_id + 1
            
            new_items = []
            for text in new_texts:
                new_item = {
                    "id": next_id,
                    "keywords": [keyword],
                    "text": text
                }
                docs.append(new_item)
                new_items.append(new_item)
                next_id += 1
            
            # íŒŒì¼ ì €ì¥
            data["documents"] = docs
            with open(docs_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            reload_documents()
            
            return {
                "success": True,
                "message": f"{len(new_texts)}ê°œ ë¬¸ì¥ì´ ìƒì„±ë˜ì–´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤",
                "items": new_items
            }
        else:
            return {
                "success": False,
                "error": f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ: {mode}"
            }
            
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": str(e),
            "detail": traceback.format_exc()
        }
